{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e9470f",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21be3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window #for ranking\n",
    "from pyspark.sql.functions import lit, mean, stddev_pop\n",
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "from pyspark.sql.functions import struct\n",
    "from pyspark.sql.functions import slice\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.types import DecimalType, ArrayType, IntegerType, FloatType\n",
    "import findspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import avg, broadcast, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76529ee7",
   "metadata": {},
   "source": [
    "Define cosine similarity and weighted avg functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d311240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining spark \n",
    "def init_spark():\n",
    "    findspark.init()\n",
    "    spark = SparkSession.builder.appName('ReadMariaDB') \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"1000000\") \\\n",
    "    .config(\"spark.jars\", \"D:/MariaDb/mariadb-java-client-3.1.3.jar\") \\\n",
    "    .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "# cosine similarity function\n",
    "def cosine_similarity_udf(a, b):\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x**2 for x in a])**0.5\n",
    "    norm_b = sum([x**2 for x in b])**0.5\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "# weighted average features function\n",
    "def weighted_avg_features(ratings, features):\n",
    "    if not ratings or not features:\n",
    "        return []\n",
    "\n",
    "    weighted_sum = [0] * len(features[0])\n",
    "    total_weight = 0\n",
    "\n",
    "    for rating, feature in zip(ratings, features):\n",
    "        weight = float(rating)\n",
    "        total_weight += weight\n",
    "        weighted_sum = [ws + weight * f for ws, f in zip(weighted_sum, feature)]\n",
    "\n",
    "    if total_weight == 0:\n",
    "        return weighted_sum\n",
    "\n",
    "    return [ws / total_weight for ws in weighted_sum]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69354ddb",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b130234",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m init_spark()\n\u001b[0;32m      2\u001b[0m database \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msteam_recommender\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m user \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36minit_spark\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_spark\u001b[39m():\n\u001b[0;32m      3\u001b[0m     findspark\u001b[39m.\u001b[39minit()\n\u001b[0;32m      4\u001b[0m     spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mReadMariaDB\u001b[39;49m\u001b[39m'\u001b[39;49m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m64g\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      6\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.sql.pivotMaxValues\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m1000000\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      7\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.jars\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mD:/MariaDb/mariadb-java-client-3.1.3.jar\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m----> 8\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m spark\n",
      "File \u001b[1;32mc:\\Users\\Legend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    270\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\Legend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    484\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\Legend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Legend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Legend\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    109\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "sc = init_spark()\n",
    "database = \"steam_recommender\"\n",
    "user = \"root\"\n",
    "password = \"example\"\n",
    "server = \"127.0.0.1\"\n",
    "port = 3306\n",
    "jdbc_url = f\"jdbc:mariadb://{server}:{port}/{database}\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT p.steamid, p.appid, p.playtime_2weeks, p.playtime_forever, p.dateretrieved, g.genre\n",
    "FROM 01_sampled_games_2v2 AS p\n",
    "JOIN games_genres AS g ON p.appid = g.appid\n",
    "WHERE p.playtime_forever IS NOT NULL AND p.playtime_forever > 0\n",
    "\"\"\"\n",
    "\n",
    "df = sc.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "# Show the data frame\n",
    "df = df.drop(\"playtime_2weeks\", \"dateretrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c922e29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# count number of rows in the dataframe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m row_count \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcount()\n\u001b[0;32m      3\u001b[0m \u001b[39m# print the row count\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDataframe has\u001b[39m\u001b[39m\"\u001b[39m, row_count, \u001b[39m\"\u001b[39m\u001b[39m rows.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# count number of rows in the dataframe\n",
    "row_count = df.count()\n",
    "# print the row count\n",
    "print(\"Dataframe has\", row_count, \" rows.\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf621a0",
   "metadata": {},
   "source": [
    "Build item profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7e7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+----------------+----------+--------------------+\n",
      "|appid|          steamid|playtime_forever|     genre|        genre_vector|\n",
      "+-----+-----------------+----------------+----------+--------------------+\n",
      "|  300|76561197960268000|             109|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 1300|76561197960268000|              94|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 2100|76561197960268000|             110|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 2100|76561197960268000|             110|       RPG|[1, 0, 0, 0, 0, 0...|\n",
      "| 4000|76561197960268000|             152|     Indie|[0, 0, 0, 0, 0, 0...|\n",
      "| 4000|76561197960268000|             152|Simulation|[0, 0, 0, 0, 0, 0...|\n",
      "| 2600|76561197960268000|              59|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 9000|76561197960268000|               2|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 2300|76561197960268000|              40|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 2200|76561197960268000|             210|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 4500|76561197960268000|            1002|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "| 4500|76561197960268000|            1002|       RPG|[1, 0, 0, 0, 0, 0...|\n",
      "|  400|76561197960268000|             380|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "|17300|76561197960268000|            4312|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "|  500|76561197960268000|             198|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "|22300|76561197960268000|             425|       RPG|[0, 0, 0, 0, 0, 0...|\n",
      "|19900|76561197960268000|             111|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "|18500|76561197960268000|            1160|     Indie|[0, 0, 0, 0, 0, 0...|\n",
      "|18500|76561197960268000|            1160|  Strategy|[0, 0, 0, 0, 0, 0...|\n",
      "|22200|76561197960268000|              20|    Action|[1, 0, 0, 0, 0, 0...|\n",
      "+-----+-----------------+----------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the item profiles\n",
    "# Group the data by 'appid' and collect the genres for each game into a list\n",
    "games_genres_df = df.groupBy(\"appid\").agg(collect_set(\"genre\").alias(\"genres\"))\n",
    "\n",
    "# Create a list of unique genres\n",
    "unique_genres = sorted(df.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "# Define a UDF to create a binary vector for each game's genres\n",
    "@udf(returnType=ArrayType(IntegerType()))\n",
    "def genre_vector(genres):\n",
    "    return [1 if genre in genres else 0 for genre in unique_genres]\n",
    "# Add a new column 'genre_vector' to the DataFrame\n",
    "# the genre vector will now have a 1 for each genre that the game belongs to\n",
    "games_genres_df = games_genres_df.withColumn(\"genre_vector\", genre_vector(\"genres\"))\n",
    "\n",
    "# games_genres_df.show(truncate=False)\n",
    "# Join the main DataFrame with the games_genres_df on appid to include the genre_vector\n",
    "df = df.join(broadcast(games_genres_df.select(\"appid\", \"genre_vector\")), on=\"appid\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf0603",
   "metadata": {},
   "source": [
    "Build the user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbc4db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+------------------------+--------------------+\n",
      "|          steamid|         genres_list|playtime_normalized_list|        user_profile|\n",
      "+-----------------+--------------------+------------------------+--------------------+\n",
      "|76561197960268000|[[0, 0, 0, 1, 0, ...|    [1.0, 1.0, 1.0, 1...|[0.69411767, 0.21...|\n",
      "|76561197960274000|[[1, 0, 0, 0, 0, ...|    [1.0, 1.0, 1.0, 1...|[0.77272725, 0.04...|\n",
      "|76561197960277000|[[1, 0, 0, 0, 0, ...|              [1.0, 1.0]|[1.0, 0.0, 0.0, 0...|\n",
      "|76561197960283000|[[1, 0, 0, 1, 0, ...|    [1.0, 1.0, 1.0, 1...|[0.375, 0.125, 0....|\n",
      "|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|\n",
      "|76561197960293000|[[0, 0, 0, 1, 0, ...|    [1.0, 1.0, 1.0, 1...|[0.21428572, 0.21...|\n",
      "|76561197960303000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|\n",
      "|76561197960306000|[[1, 0, 0, 0, 0, ...|    [1.0, 1.0, 1.0, 1...|[0.5744681, 0.276...|\n",
      "|76561197960324000|[[1, 0, 0, 0, 0, ...|              [1.0, 1.0]|[0.5, 0.0, 0.0, 0...|\n",
      "|76561197960328000|[[0, 0, 0, 0, 0, ...|                   [1.0]|[0.0, 0.0, 0.0, 0...|\n",
      "|76561197960329000|[[1, 0, 0, 0, 0, ...|         [1.0, 1.0, 1.0]|[0.6666667, 0.0, ...|\n",
      "|76561197960341000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|\n",
      "|76561197960355000|[[1, 0, 0, 0, 0, ...|         [1.0, 1.0, 1.0]|[0.6666667, 0.333...|\n",
      "|76561197960364000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|\n",
      "|76561197960370000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|\n",
      "|76561197960378000|[[0, 1, 0, 1, 0, ...|                   [1.0]|[0.0, 1.0, 0.0, 1...|\n",
      "|76561197960409000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|\n",
      "|76561197960417000|[[0, 0, 0, 1, 0, ...|    [1.0, 1.0, 1.0, 1...|[0.33333334, 0.26...|\n",
      "|76561197960421000|[[0, 0, 0, 0, 0, ...|                   [1.0]|[0.0, 0.0, 0.0, 0...|\n",
      "|76561197960444000|[[0, 0, 0, 1, 0, ...|                   [1.0]|[0.0, 0.0, 0.0, 1...|\n",
      "+-----------------+--------------------+------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate the global playtime average for each game\n",
    "global_playtime_avg = df.groupBy(\"appid\").agg(avg(\"playtime_forever\").alias(\"global_playtime_avg\"))\n",
    "\n",
    "# 2. Normalize the user's playtime for each game based on the global average\n",
    "df = df.join(broadcast(global_playtime_avg), on=\"appid\")\n",
    "df = df.withColumn(\"playtime_normalized\", F.when(df.playtime_forever == 0, 1).otherwise(df.playtime_forever / df.global_playtime_avg))\n",
    "\n",
    "# 3. Implement the user profile\n",
    "# First, let's group the data by user and aggregate the genre vectors and normalized playtimes\n",
    "user_aggregated_data = df.groupBy(\"steamid\").agg(\n",
    "    collect_list(\"genre_vector\").alias(\"genres_list\"),\n",
    "    collect_list(\"playtime_normalized\").alias(\"playtime_normalized_list\")\n",
    ")\n",
    "\n",
    "# Now, let's define a UDF to calculate the weighted average of genre vectors\n",
    "weighted_avg_features_udf = udf(weighted_avg_features, ArrayType(FloatType()))\n",
    "\n",
    "# Calculate the user profile as the weighted average of rated item profiles (genre vectors)\n",
    "user_profiles = user_aggregated_data.withColumn(\"user_profile\", weighted_avg_features_udf(\"playtime_normalized_list\", \"genres_list\"))\n",
    "\n",
    "user_profiles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122cace",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9840bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+-----------------+--------------------+------------------------+--------------------+----------+\n",
      "|appid|         genres|        genre_vector|          steamid|         genres_list|playtime_normalized_list|        user_profile|similarity|\n",
      "+-----+---------------+--------------------+-----------------+--------------------+------------------------+--------------------+----------+\n",
      "| 4300|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|42500|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|19200|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|22200|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|26000|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|26300|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|31700|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|36000|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|39800|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "|41000|[Action, Indie]|[1, 0, 0, 0, 0, 0...|76561197960288000|[[1, 0, 0, 0, 0, ...|                   [1.0]|[1.0, 0.0, 0.0, 0...|       1.0|\n",
      "+-----+---------------+--------------------+-----------------+--------------------+------------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction heuristics\n",
    "# calculate cosine distance of an item and user profile\n",
    "\n",
    "# 1. create udf for cosine similarity\n",
    "cosine_similarity = udf(cosine_similarity_udf, FloatType())\n",
    "# cross join the game_genres_df with the user_profiles\n",
    "cross_joined = games_genres_df.crossJoin(user_profiles)\n",
    "\n",
    "# calculate the cosine similarity between each item and user\n",
    "recommendations = cross_joined.withColumn(\n",
    "    \"similarity\", cosine_similarity(\"genre_vector\", \"user_profile\")\n",
    ")\n",
    "\n",
    "# sort based on similarity score\n",
    "sorted_recommendations = recommendations.sort(desc(\"similarity\"))\n",
    "\n",
    "sorted_recommendations.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55392e6",
   "metadata": {},
   "source": [
    "recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb52bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create a window by steamid and similarity to get ranking\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m window_spec \u001b[39m=\u001b[39m Window\u001b[39m.\u001b[39mpartitionBy(\u001b[39m\"\u001b[39m\u001b[39msteamid\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39morderBy(desc(\u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m ranked_recommendations \u001b[39m=\u001b[39m sorted_recommendations\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m\"\u001b[39m, F\u001b[39m.\u001b[39mrow_number()\u001b[39m.\u001b[39mover(window_spec))\n\u001b[0;32m      6\u001b[0m top_10_recommendations \u001b[39m=\u001b[39m ranked_recommendations\u001b[39m.\u001b[39mfilter(ranked_recommendations\u001b[39m.\u001b[39mrank \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Window' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a window by steamid and similarity to get ranking\n",
    "window_spec = Window.partitionBy(\"steamid\").orderBy(desc(\"similarity\"))\n",
    "\n",
    "ranked_recommendations = sorted_recommendations.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "\n",
    "top_10_recommendations = ranked_recommendations.filter(ranked_recommendations.rank <= 10)\n",
    "top_10_recommendations.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4091eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
