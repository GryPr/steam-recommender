{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-item collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when, udf, stddev_pop, col, lit, collect_list, avg, stddev, collect_list, udf, first\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import math\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Load dataset\n",
    "2. Split the dataset\n",
    "3. Normalize the dataset playtime into a 1-5 rating scale\n",
    "4. Compute Pearson correlation matrix\n",
    "5. Rating prediction by doing a weighted average of the k most similar items that the user has played before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "The dataset is loaded from MariaDB into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ReadMariaDB') \\\n",
    ".config(\"spark.driver.memory\", \"32g\") \\\n",
    ".config(\"spark.sql.pivotMaxValues\", \"1000000\") \\\n",
    ".config('spark.sql.codegen.wholeStage', 'false')\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "sql = \"select * from 01_sampled_games_2 WHERE playtime_forever IS NOT NULL\"\n",
    "database = \"steam\"\n",
    "user = \"root\"\n",
    "password = \"example\"\n",
    "server = \"127.0.0.1\"\n",
    "port = 3306\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "# Create a data frame by reading data from Oracle via JDBC\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "df = df.drop(\"playtime_2weeks\", \"dateretrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 712806 rows.\n",
      "+-----------------+-----+----------------+\n",
      "|          steamid|appid|playtime_forever|\n",
      "+-----------------+-----+----------------+\n",
      "|76561197960268000|   10|               8|\n",
      "|76561197960268000|   20|               1|\n",
      "|76561197960268000|   30|               0|\n",
      "|76561197960268000|   40|               0|\n",
      "|76561197960268000|   50|            1719|\n",
      "|76561197960268000|   60|               1|\n",
      "|76561197960268000|   70|            1981|\n",
      "|76561197960268000|  130|             175|\n",
      "|76561197960268000|  220|            3873|\n",
      "|76561197960268000|  240|             221|\n",
      "|76561197960268000|  320|               1|\n",
      "|76561197960268000|  340|               0|\n",
      "|76561197960268000|   80|               0|\n",
      "|76561197960268000|  100|               0|\n",
      "|76561197960268000|  280|            1242|\n",
      "|76561197960268000|  300|             109|\n",
      "|76561197960268000|  360|               3|\n",
      "|76561197960268000| 1300|              94|\n",
      "|76561197960268000| 1309|               0|\n",
      "|76561197960268000| 1313|             213|\n",
      "+-----------------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"The DataFrame has\", row_count, \"rows.\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Game 1-5 Rating Normalization\n",
    "For each game, we calculate the mean and standard deviation. We then create buckets for each rating:\n",
    "### Scaling factor\n",
    "The cut points are scaled on a per-user basis since some users are more casual gamers while others may spend a lot more time gaming. The scaling factor is calculated as follows:\n",
    "\n",
    "(user_playtime_average)/(global_playtime_average)\n",
    "\n",
    "### Cut points\n",
    "* Cut point 1: 30\n",
    "* Cut point 2: 100\n",
    "* Cut point 3: 200\n",
    "* Cut point 4: 500\n",
    "\n",
    "### Ratings\n",
    "* Rating 1: 0 < x < cut point 1\n",
    "* Rating 2: cut point 1 < x < cut point 2\n",
    "* Rating 3: cut point 2 < x < cut point 3\n",
    "* Rating 4: cut point 3 < x < cut point 4\n",
    "* Rating 5: cut point 5 < x < inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset_v2(df):\n",
    "    # Bucketize the playtime_forever column with intervals 0-30, 30-70, 70-200, 200-500, 500-inf\n",
    "    return df.withColumn(\n",
    "        \"ratings\",\n",
    "        when(col(\"playtime_forever\") <= 30, lit(1))\n",
    "        .when((col(\"playtime_forever\") > 30) & (col(\"playtime_forever\") <= 100), lit(2))\n",
    "        .when((col(\"playtime_forever\") > 100) & (col(\"playtime_forever\") <= 200), lit(3))\n",
    "        .when((col(\"playtime_forever\") > 200) & (col(\"playtime_forever\") <= 500), lit(4))\n",
    "        .otherwise(lit(5))\n",
    "    )\n",
    "\n",
    "df = normalize_dataset_v2(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into 70% training and 30% test data\n",
    "training, test = df.randomSplit([0.9, 0.1], seed=2313)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Matrix\n",
    "Since the dataset contains at most ~4500 games, we can expect a 4500^2=81,000,000 sized matrix. Each float entry takes 4 bytes in memory. Therefore, the pearson correlation matrix would take up 324 MB of memory.\n",
    "\n",
    "Since the memory used is relatively small, we will pre-compute the person correlation matrix and store it for use later in the algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we create a list of features for each game (appid) using the playtime_forever of all users who have played the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_matrix = training.groupBy(\"steamid\").pivot(\"appid\").agg(first(\"ratings\")).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create features vector\n",
    "assembler = VectorAssembler(inputCols=training_matrix.columns[1:], outputCol=\"features\")\n",
    "vector = assembler.transform(training_matrix).select(\"features\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "pearson_matrix = Correlation.corr(vector, \"features\", \"pearson\")\n",
    "corr_array = pearson_matrix.head()[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict of appid to index\n",
    "appid_index = {}\n",
    "for i, appid in enumerate(training_matrix.columns[1:]):\n",
    "    appid_index[int(appid)] = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Prediction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given user (steamid) and game (appid), we can compute the predicted rating of the game (appid) for the user (steamid) using the weighted average of the k most similar games to the game (appid) that the user (steamid) has played.\n",
    "\n",
    "This is the same item-item collaborative filtering formula that was shown in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "global_avg = training.select(avg(\"ratings\")).collect()[0][0]\n",
    "\n",
    "def predict_rating(appid, user_ratings_dict, k=3, min_k=1):\n",
    "\n",
    "    # Get appid row number from the vectors dataframe\n",
    "    if appid in appid_index:\n",
    "        appid_row_num = appid_index[appid]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "    # Get a list of correlations between the appid and all other games\n",
    "    corr = corr_array[appid_row_num]\n",
    "\n",
    "    # Create a dict of appids and correlations\n",
    "    corr_dict = {}\n",
    "    for appid in appid_index:\n",
    "        if appid in user_ratings_dict:\n",
    "            corr_dict[appid] = corr[appid_index[appid]]\n",
    "\n",
    "    # Make a list of tuples of (appid, correlation, rating)\n",
    "    corr_list = []\n",
    "    for appid, corr in corr_dict.items():\n",
    "        # Only add the appid to the list if the user has rated it\n",
    "        if appid in user_ratings_dict:\n",
    "            corr_list.append((appid, corr, user_ratings_dict[appid]))\n",
    "\n",
    "    # Sort the list by correlation\n",
    "    corr_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top k most similar appids\n",
    "    top_k = corr_list[1:k+1]\n",
    "\n",
    "    # Remove negative correlations\n",
    "    top_k = [x for x in top_k if x[1] > 0]\n",
    "\n",
    "    # If there are not enough similar appids, return the global average\n",
    "    if len(top_k) < min_k:\n",
    "        return global_avg\n",
    "\n",
    "    # Calculate the weighted average of the top 10 appids\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "    for appid, corr, rating in top_k:\n",
    "        numerator += corr * rating\n",
    "        denominator += corr\n",
    "    if denominator != 0:\n",
    "        return float(numerator / denominator)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def predict_single_rating(steamid, appid, training_df):\n",
    "    # Get all the user's ratings\n",
    "    user_ratings_dict = training_df.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "\n",
    "    # Predict the user's rating for the appid\n",
    "    return predict_rating(appid, user_ratings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for steamid [76561198023872000] and appid [500]: 3.1813717637670162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test the function for a given user and game\n",
    "test_steamid = 76561198023872000\n",
    "test_appid = 500\n",
    "print(f'Predicted rating for steamid [{test_steamid}] and appid [{test_appid}]: {predict_single_rating(test_steamid, test_appid, training)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run in parallel the rating prediction for each game that the user has not played before. We then sort the games by the predicted rating and return the top k games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get all unique appids\n",
    "all_appids = training.select(\"appid\").distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:=============>                                         (4 + 12) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted top 3 games for steamid [76561198023872000]: [Decimal('216150'), Decimal('298600'), Decimal('267220')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def predict_user_ratings(steamid, recommendation_count=3):\n",
    "    # Get all_appids that the user has not rated\n",
    "    user_ratings_dict = training.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "    not_rated = [appid for appid in all_appids if appid not in user_ratings_dict]\n",
    "    not_rated_rdd = spark.sparkContext.parallelize(not_rated)\n",
    "\n",
    "    # Run predict_rating for each appid and output a list of tuples of (appid, predicted rating)\n",
    "    predictions = not_rated_rdd.map(lambda appid: (appid, predict_rating(appid, user_ratings_dict))).collect()\n",
    "\n",
    "    # Sort the list by predicted rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top 3 appids outside of the tuple\n",
    "    return [appid for appid, rating in predictions[:recommendation_count]]\n",
    "\n",
    "print(f'Predicted top 3 games for steamid [{test_steamid}]: {predict_user_ratings(test_steamid)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the test dataset\n",
    "test_normalized = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.6294799711267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 164:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of ratings: 1.6670507946114101\n",
      "Percent difference: -2.2537299766842347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create dataframe with only one row per steamid, a list of appids and ratings from the training set and a list of appids and ratings from the test set\n",
    "from pyspark.sql.functions import size, sum\n",
    "\n",
    "# Put training in a single partition\n",
    "training = training.repartition(1)\n",
    "\n",
    "# Put test_normalized in a single partition\n",
    "test_normalized = test_normalized.repartition(1)\n",
    "\n",
    "\n",
    "steamid_ratings = training.groupBy(\"steamid\").agg(collect_list(\"appid\").alias(\"train_appids\"), collect_list(\"ratings\").alias(\"train_ratings\"))\n",
    "steamid_ratings = steamid_ratings.join(test_normalized.groupBy(\"steamid\").agg(collect_list(\"appid\").alias(\"test_appids\"), collect_list(\"ratings\").alias(\"test_ratings\")), on=\"steamid\", how=\"inner\")\n",
    "\n",
    "\n",
    "def predict_target_ratings(train_appids, train_ratings, test_appids):\n",
    "    # If the test_appids list is empty, return an empty list\n",
    "    if len(test_appids) == 0:\n",
    "        return []\n",
    "    # Get dictionary of appids and ratings from train_appids and train_ratings\n",
    "    train_ratings_dict = dict(zip(train_appids, train_ratings))\n",
    "\n",
    "    # For each appid in test_appids, do predict_rating and return a list of tuples of (appid, predicted rating)\n",
    "    predictions = [predict_rating(appid, train_ratings_dict) for appid in test_appids]\n",
    "    return predictions\n",
    "\n",
    "# Create a udf for predict_target_ratings\n",
    "predict_target_ratings_udf = udf(predict_target_ratings, ArrayType(FloatType()))\n",
    "\n",
    "# For each row in the steamid_ratings dataframe, run predict_target_ratings and add the predictions list into a new column\n",
    "steamid_ratings = steamid_ratings.withColumn(\"predictions\", predict_target_ratings_udf(\"train_appids\", \"train_ratings\", \"test_appids\"))\n",
    "\n",
    "# For each column in the predictions column, calculate the rmse\n",
    "def calculate_rmse(predictions, test_ratings):\n",
    "    # If the predictions list is empty, return 0.0\n",
    "    if len(predictions) == 0:\n",
    "        return 0.0\n",
    "    # Calculate the rmse for the predictions list\n",
    "    meansquare = 0.0\n",
    "    prediction_num = 0.0\n",
    "    for prediction, test_rating in zip(predictions, test_ratings):\n",
    "        # Check if prediction is not 0.0\n",
    "        if prediction != 0.0:\n",
    "            meansquare += (prediction - test_rating) ** 2\n",
    "            prediction_num += 1\n",
    "    return [meansquare, prediction_num]\n",
    "\n",
    "# Create a udf for calculate_rmse\n",
    "calculate_rmse_udf = udf(calculate_rmse, ArrayType(FloatType()))\n",
    "\n",
    "# For each row in the steamid_ratings dataframe, run calculate_rmse and add the rmse into a new column\n",
    "steamid_ratings = steamid_ratings.withColumn(\"meansquare\", calculate_rmse_udf(\"predictions\", \"test_ratings\"))\n",
    "\n",
    "# Split the meansquare column into two columns\n",
    "steamid_ratings = steamid_ratings.withColumn(\"ms\", col(\"meansquare\")[0])\n",
    "steamid_ratings = steamid_ratings.withColumn(\"prediction_num\", col(\"meansquare\")[1])\n",
    "steamid_ratings = steamid_ratings.drop(\"meansquare\")\n",
    "\n",
    "# Sort steamid_ratings by ms\n",
    "steamid_ratings = steamid_ratings.sort(col(\"ms\").desc())\n",
    "\n",
    "# Get the sum of the ms column\n",
    "ms_sum = steamid_ratings.select(sum(\"ms\")).collect()[0][0]\n",
    "\n",
    "# Get the sum of the prediction_num column\n",
    "prediction_num_sum = steamid_ratings.select(sum(\"prediction_num\")).collect()[0][0]\n",
    "\n",
    "rmse = math.sqrt(ms_sum / prediction_num_sum)\n",
    "\n",
    "# Print the rmse\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Calculate the standard deviation of the ratings\n",
    "ratings_std = test_normalized.select(stddev(\"ratings\")).collect()[0][0]\n",
    "print(f'Standard deviation of ratings: {ratings_std}')\n",
    "\n",
    "# Calculate the percent difference between the rmse and the standard deviation\n",
    "percent_diff = (rmse - ratings_std) / ratings_std * 100\n",
    "print(f'Percent difference: {percent_diff}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5136604333185304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create rdd of all appids\n",
    "from typing import List\n",
    "\n",
    "\n",
    "all_appids_rdd = spark.sparkContext.parallelize(all_appids)\n",
    "\n",
    "def predict_all_user_ratings(train_appids, train_ratings, test_appids, test_ratings, k=10):\n",
    "    # Return 0 if any of the lists are empty\n",
    "    if len(train_appids) == 0 or len(train_ratings) == 0 or len(test_appids) == 0 or len(test_ratings) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    all_appids = [*train_appids, *test_appids]\n",
    "    all_ratings = [*train_ratings, *test_ratings]\n",
    "\n",
    "    # If the length of all_appids is less than k, return -1.0\n",
    "    if len(all_appids) < k:\n",
    "        return -1.0\n",
    "\n",
    "    # Create list of tuples of (appid, rating)\n",
    "    appid_ratings = list(zip(all_appids, all_ratings))\n",
    "\n",
    "    # Sort tuple list by rating in descending order\n",
    "    appid_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top k appids\n",
    "    top_k_appids = [appid for appid, rating in appid_ratings[:k]]\n",
    "\n",
    "    # Create dictionary of appids and ratings\n",
    "    user_ratings_dict = dict(appid_ratings)\n",
    "\n",
    "    # For each appid in test_appids, do predict_rating and return a list of tuples of (appid, predicted rating)\n",
    "    predictions = [(appid, predict_rating(appid, user_ratings_dict)) for appid in all_appids]\n",
    "    \n",
    "    # Sort the list by predicted rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top k appids outside of the tuple\n",
    "    k_recommendations = [appid for appid, rating in predictions[:k]]\n",
    "\n",
    "    # Calculate the proportion of recommendations that are in the top k appids\n",
    "    precision = len(set(top_k_appids).intersection(set(k_recommendations))) / k\n",
    "\n",
    "    return precision\n",
    "\n",
    "# Create udf for predict_all_user_ratings\n",
    "predict_all_user_ratings_udf = udf(predict_all_user_ratings, FloatType())\n",
    "\n",
    "# For each row in the steamid_ratings dataframe, run predict_all_user_ratings and add the predictions list into a new column\n",
    "precision_df = steamid_ratings.withColumn(\"precision\", predict_all_user_ratings_udf(\"train_appids\", \"train_ratings\", \"test_appids\", \"test_ratings\"))\n",
    "\n",
    "# Remove rows where precision is -1.0\n",
    "precision_df = precision_df.filter(precision_df.precision != -1.0)\n",
    "\n",
    "# Average the precision column\n",
    "precision = precision_df.select(avg(\"precision\")).collect()[0][0]\n",
    "print(f'Precision: {precision}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34e120910b4957e5cd755e2a42b93193ccb3c34461c3543db7ce9f360eefaa4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
