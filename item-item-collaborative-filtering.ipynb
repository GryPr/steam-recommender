{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-item collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, when, udf, stddev_pop, col, lit, collect_list, avg, stddev, collect_list, udf, first\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import math\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Load dataset\n",
    "2. Split the dataset\n",
    "3. Normalize the dataset playtime into a 1-5 rating scale\n",
    "4. Compute Pearson correlation matrix\n",
    "5. Rating prediction by doing a weighted average of the k most similar items that the user has played before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "The dataset is loaded from MariaDB into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ReadMariaDB') \\\n",
    ".config(\"spark.driver.memory\", \"32g\") \\\n",
    ".config(\"spark.sql.pivotMaxValues\", \"1000000\") \\\n",
    ".config('spark.sql.codegen.wholeStage', 'false')\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "sql = \"select * from 01_sampled_games_2 WHERE playtime_forever IS NOT NULL AND playtime_forever > 0\"\n",
    "database = \"steam\"\n",
    "user = \"root\"\n",
    "password = \"example\"\n",
    "server = \"127.0.0.1\"\n",
    "port = 3306\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "# Create a data frame by reading data from Oracle via JDBC\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "df = df.drop(\"playtime_2weeks\", \"dateretrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 408349 rows.\n",
      "+-----------------+-----+----------------+\n",
      "|          steamid|appid|playtime_forever|\n",
      "+-----------------+-----+----------------+\n",
      "|76561197960268000|   10|               8|\n",
      "|76561197960268000|   20|               1|\n",
      "|76561197960268000|   50|            1719|\n",
      "|76561197960268000|   60|               1|\n",
      "|76561197960268000|   70|            1981|\n",
      "|76561197960268000|  130|             175|\n",
      "|76561197960268000|  220|            3873|\n",
      "|76561197960268000|  240|             221|\n",
      "|76561197960268000|  320|               1|\n",
      "|76561197960268000|  280|            1242|\n",
      "|76561197960268000|  300|             109|\n",
      "|76561197960268000|  360|               3|\n",
      "|76561197960268000| 1300|              94|\n",
      "|76561197960268000| 1313|             213|\n",
      "|76561197960268000|  380|             944|\n",
      "|76561197960268000| 2100|             110|\n",
      "|76561197960268000| 4000|             152|\n",
      "|76561197960268000| 3970|             586|\n",
      "|76561197960268000| 2600|              59|\n",
      "|76561197960268000| 6910|            1729|\n",
      "+-----------------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"The DataFrame has\", row_count, \"rows.\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Game 1-5 Rating Normalization\n",
    "For each game, we calculate the mean and standard deviation. We then create buckets for each rating:\n",
    "### Scaling factor\n",
    "The cut points are scaled on a per-user basis since some users are more casual gamers while others may spend a lot more time gaming. The scaling factor is calculated as follows:\n",
    "\n",
    "(user_playtime_average)/(global_playtime_average)\n",
    "\n",
    "### Cut points\n",
    "* Cut point 1: (mean - std_dev*0.5) * scaling_factor if > 0, else 0\n",
    "* Cut point 2: mean\n",
    "* Cut point 3: (mean + std_dev*0.5) * scaling_factor\n",
    "* Cut point 4: (mean + std_dev) * scaling_factor\n",
    "### Ratings\n",
    "* Rating 1: 0 < x < cut point 1\n",
    "* Rating 2: cut point 1 < x < cut point 2\n",
    "* Rating 3: cut point 2 < x < cut point 3\n",
    "* Rating 4: cut point 3 < x < cut point 4\n",
    "* Rating 5: cut point 5 < x < inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(df):\n",
    "    # Calculate the per-game mean and standard deviation of the playtime column\n",
    "    game_stats = df.filter(col(\"playtime_forever\") > 0).groupBy(\"appid\").agg(\n",
    "        mean(\"playtime_forever\").alias(\"game_mean_playtime\"),\n",
    "        stddev_pop(\"playtime_forever\").alias(\"game_stddev_playtime\")\n",
    "    )\n",
    "\n",
    "    # Calculate the overall playtime average\n",
    "    overall_playtime_avg = df.filter(col(\"playtime_forever\") > 0).agg(avg(\"playtime_forever\")).collect()[0][0]\n",
    "\n",
    "    # Calculate the per-steamid playtime average\n",
    "    user_playtime_avg = df.filter(col(\"playtime_forever\") > 0).groupBy(\"steamid\").agg(avg(\"playtime_forever\")).withColumnRenamed(\"avg(playtime_forever)\", \"user_playtime_avg\")\n",
    "\n",
    "    # Join the user_playtime_avg dataframe with the main dataframe\n",
    "    df = df.join(user_playtime_avg, \"steamid\")\n",
    "\n",
    "    # Join the game_stats dataframe with the main dataframe\n",
    "    df = df.join(game_stats, \"appid\")\n",
    "\n",
    "    # Calculate the scaling factor based on the ratio of user playtime to overall playtime\n",
    "    df = df.withColumn(\"scaling_factor\", col(\"user_playtime_avg\") / overall_playtime_avg)\n",
    "\n",
    "    # Calculate the adjusted cut points\n",
    "    df = df.withColumn(\"cut_point_1\", when((col(\"game_mean_playtime\") - col(\"game_stddev_playtime\") * 0.5) * col(\"scaling_factor\") > 0, col(\"game_mean_playtime\") - (col(\"game_stddev_playtime\") * 0.5 * col(\"scaling_factor\"))).otherwise(0))\n",
    "    df = df.withColumn(\"cut_point_2\", col(\"game_mean_playtime\") * col(\"scaling_factor\"))\n",
    "    df = df.withColumn(\"cut_point_3\", (col(\"game_mean_playtime\") + col(\"game_stddev_playtime\") * 0.5) * col(\"scaling_factor\"))\n",
    "    df = df.withColumn(\"cut_point_4\", (col(\"game_mean_playtime\") + col(\"game_stddev_playtime\")) * col(\"scaling_factor\"))\n",
    "\n",
    "    # Assign ratings based on adjusted cut points\n",
    "    df = df.withColumn(\n",
    "        \"ratings\",\n",
    "        when(col(\"playtime_forever\") <= col(\"cut_point_1\"), lit(1))\n",
    "        .when((col(\"playtime_forever\") > col(\"cut_point_1\")) & (col(\"playtime_forever\") <= col(\"cut_point_2\")), lit(2))\n",
    "        .when((col(\"playtime_forever\") > col(\"cut_point_2\")) & (col(\"playtime_forever\") <= col(\"cut_point_3\")), lit(3))\n",
    "        .when((col(\"playtime_forever\") > col(\"cut_point_3\")) & (col(\"playtime_forever\") <= col(\"cut_point_4\")), lit(4))\n",
    "        .otherwise(lit(5))\n",
    "    )\n",
    "\n",
    "    # Drop the columns that are no longer needed\n",
    "    df = df.drop(\"playtime_forever\", \"game_mean_playtime\", \"game_stddev_playtime\", \"user_playtime_avg\", \"scaling_factor\", \"cut_point_1\", \"cut_point_2\", \"cut_point_3\", \"cut_point_4\")\n",
    "    return df\n",
    "\n",
    "df = normalize_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into 70% training and 30% test data\n",
    "training, test = df.randomSplit([0.9, 0.1], seed=2313)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Matrix\n",
    "Since the dataset contains at most ~4500 games, we can expect a 4500^2=81,000,000 sized matrix. Each float entry takes 4 bytes in memory. Therefore, the pearson correlation matrix would take up 324 MB of memory.\n",
    "\n",
    "Since the memory used is relatively small, we will pre-compute the person correlation matrix and store it for use later in the algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we create a list of features for each game (appid) using the playtime_forever of all users who have played the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_matrix = training.groupBy(\"steamid\").pivot(\"appid\").agg(first(\"ratings\")).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create features vector\n",
    "assembler = VectorAssembler(inputCols=training_matrix.columns[1:], outputCol=\"features\")\n",
    "vector = assembler.transform(training_matrix).select(\"features\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "pearson_matrix = Correlation.corr(vector, \"features\", \"pearson\")\n",
    "corr_array = pearson_matrix.head()[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict of appid to index\n",
    "appid_index = {}\n",
    "for i, appid in enumerate(training_matrix.columns[1:]):\n",
    "    appid_index[int(appid)] = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Prediction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given user (steamid) and game (appid), we can compute the predicted rating of the game (appid) for the user (steamid) using the weighted average of the k most similar games to the game (appid) that the user (steamid) has played.\n",
    "\n",
    "This is the same item-item collaborative filtering formula that was shown in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg = training.select(avg(\"ratings\")).collect()[0][0]\n",
    "\n",
    "def predict_rating(appid, user_ratings_dict, k=3, min_k=1):\n",
    "\n",
    "    # Get appid row number from the vectors dataframe\n",
    "    if appid in appid_index:\n",
    "        appid_row_num = appid_index[appid]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "    # Get a list of correlations between the appid and all other games\n",
    "    corr = corr_array[appid_row_num]\n",
    "\n",
    "    # Create a dict of appids and correlations\n",
    "    corr_dict = {}\n",
    "    for appid in appid_index:\n",
    "        if appid in user_ratings_dict:\n",
    "            corr_dict[appid] = corr[appid_index[appid]]\n",
    "\n",
    "    # Make a list of tuples of (appid, correlation, rating)\n",
    "    corr_list = []\n",
    "    for appid, corr in corr_dict.items():\n",
    "        # Only add the appid to the list if the user has rated it\n",
    "        if appid in user_ratings_dict:\n",
    "            corr_list.append((appid, corr, user_ratings_dict[appid]))\n",
    "\n",
    "    # Sort the list by correlation\n",
    "    corr_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top k most similar appids\n",
    "    top_k = corr_list[1:k+1]\n",
    "\n",
    "    # Remove negative correlations\n",
    "    top_k = [x for x in top_k if x[1] > 0]\n",
    "\n",
    "    # If there are not enough similar appids, return the global average\n",
    "    if len(top_k) < min_k:\n",
    "        return global_avg\n",
    "\n",
    "    # Calculate the weighted average of the top 10 appids\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "    for appid, corr, rating in top_k:\n",
    "        numerator += corr * rating\n",
    "        denominator += corr\n",
    "    if denominator != 0:\n",
    "        return float(numerator / denominator)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def predict_single_rating(steamid, appid, training_df):\n",
    "    # Get all the user's ratings\n",
    "    user_ratings_dict = training_df.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "\n",
    "    # Predict the user's rating for the appid\n",
    "    return predict_rating(appid, user_ratings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for steamid [76561198023872000] and appid [500]: 2.299157668319233\n"
     ]
    }
   ],
   "source": [
    "# Test the function for a given user and game\n",
    "test_steamid = 76561198023872000\n",
    "test_appid = 500\n",
    "print(f'Predicted rating for steamid [{test_steamid}] and appid [{test_appid}]: {predict_single_rating(test_steamid, test_appid, training)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run in parallel the rating prediction for each game that the user has not played before. We then sort the games by the predicted rating and return the top k games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get all unique appids\n",
    "all_appids = training.select(\"appid\").distinct().rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted top 3 games for steamid [76561198023872000]: [Decimal('46600'), Decimal('299440'), Decimal('266550')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def predict_user_ratings(steamid, recommendation_count=3):\n",
    "    # Get all_appids that the user has not rated\n",
    "    user_ratings_dict = training.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "    not_rated = [appid for appid in all_appids if appid not in user_ratings_dict]\n",
    "    not_rated_rdd = spark.sparkContext.parallelize(not_rated)\n",
    "\n",
    "    # Run predict_rating for each appid and output a list of tuples of (appid, predicted rating)\n",
    "    predictions = not_rated_rdd.map(lambda appid: (appid, predict_rating(appid, user_ratings_dict))).collect()\n",
    "\n",
    "    # Sort the list by predicted rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top 3 appids outside of the tuple\n",
    "    return [appid for appid, rating in predictions[:recommendation_count]]\n",
    "\n",
    "print(f'Predicted top 3 games for steamid [{test_steamid}]: {predict_user_ratings(test_steamid)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the test dataset\n",
    "test_normalized = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|          steamid|        train_appids|       train_ratings|         test_appids|        test_ratings|         predictions|      rmse|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|76561197960334000|[70, 80, 220, 240...|[3, 4, 1, 2, 4, 2...|       [2310, 35700]|              [5, 1]|[3.3010597, 2.103...|  2.052434|\n",
      "|76561197960342000|[20, 30, 70, 240,...|[2, 3, 2, 2, 2, 2...|     [24200, 108710]|              [2, 1]|[2.6842768, 2.352...| 1.1489708|\n",
      "|76561197960816000|[570, 41010, 4268...|[2, 1, 3, 2, 5, 5...|    [220440, 226320]|              [1, 2]|[2.8471413, 2.207...| 1.7274506|\n",
      "|76561197960884000|[240, 440, 630, 7...|[2, 2, 2, 2, 2, 5...|[24200, 49520, 21...|        [3, 1, 1, 2]|[2.3153758, 1.310...| 2.5755517|\n",
      "|76561197961366000|[30, 60, 70, 211,...|[3, 2, 2, 2, 2, 2...|[240, 24740, 4410...|[5, 3, 1, 1, 3, 2...|[2.0, 2.3317957, ...| 1.6324418|\n",
      "|76561197961539000|[130, 220, 340, 6...|     [2, 1, 5, 3, 2]|               [240]|                 [3]|         [1.8610132]|  1.297291|\n",
      "|76561197961590000|[80, 240, 440, 73...|[2, 4, 2, 3, 2, 2...|                [10]|                 [2]|         [3.3325288]| 1.7756331|\n",
      "|76561197961685000|[30, 730, 8870, 1...|[2, 2, 1, 1, 1, 1...|[570, 620, 39160,...|[4, 1, 2, 1, 2, 1...|[2.2947605, 1.0, ...| 0.8249095|\n",
      "|76561197961870000|[240, 380, 8190, ...|[2, 2, 5, 2, 2, 2...|                [10]|                 [2]|         [2.3389025]|0.11485489|\n",
      "|76561197962090000|[20, 30, 130, 630...|[2, 2, 2, 2, 2, 2...|      [20700, 35700]|              [1, 5]|[3.0610583, 4.033...| 2.5912955|\n",
      "|76561197962414000|[60, 440, 2790, 2...|[2, 3, 5, 5, 1, 3...|               [400]|                 [2]|         [4.4474273]|    5.9899|\n",
      "|76561197962565000|[30, 70, 240, 300...|[2, 2, 2, 2, 3, 2...|        [630, 42710]|              [2, 2]|[2.5224438, 2.381...|0.20909342|\n",
      "|76561197962963000|[30, 240, 19900, 10]|        [2, 2, 5, 2]|          [280, 730]|              [1, 2]|[3.5286367, 2.925...| 3.6256676|\n",
      "|76561197963250000|[80, 220, 240, 38...|[2, 2, 2, 3, 2, 3...|[40800, 42640, 42...|[2, 2, 2, 2, 2, 2...|[2.0, 2.0, 2.6107...|0.13631098|\n",
      "|76561197963582000|[220, 240, 440, 5...|[1, 2, 2, 4, 3, 2...|[300, 7940, 17460...|[2, 2, 5, 2, 1, 5...|[3.3574984, 3.0, ...|  2.872812|\n",
      "|76561197963670000|[340, 440, 570, 6...|[2, 2, 2, 1, 3, 2...|              [7760]|                 [1]|         [2.4622686]| 2.1382294|\n",
      "|76561197963807000|[550, 730, 33930,...|  [2, 3, 2, 2, 2, 2]|[240, 440, 4000, 30]|        [2, 2, 2, 2]|[2.0, 2.5209103, ...|0.14102638|\n",
      "|76561197963977000|[20, 30, 60, 70, ...|  [2, 2, 2, 2, 5, 2]|               [240]|                 [3]|               [2.0]|       1.0|\n",
      "|76561197964222000|[60, 400, 570, 62...|[3, 4, 2, 1, 1, 1...|[3260, 6910, 1612...|[1, 1, 1, 1, 1, 1...|[1.715237, 3.8327...|  1.816701|\n",
      "|76561197964278000|[10540, 42690, 42...|     [2, 2, 2, 3, 3]|             [42680]|                 [2]|         [2.4936275]|0.24366815|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4075714808907511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of ratings: 1.2920454355471376\n",
      "Percent difference: 8.941329938191544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 316:===================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of rmse: 2.1222401181322894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create dataframe with only one row per steamid, a list of appids and ratings from the training set and a list of appids and ratings from the test set\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "\n",
    "steamid_ratings = training.groupBy(\"steamid\").agg(collect_list(\"appid\").alias(\"train_appids\"), collect_list(\"ratings\").alias(\"train_ratings\"))\n",
    "steamid_ratings = steamid_ratings.join(test_normalized.groupBy(\"steamid\").agg(collect_list(\"appid\").alias(\"test_appids\"), collect_list(\"ratings\").alias(\"test_ratings\")), on=\"steamid\", how=\"inner\")\n",
    "\n",
    "\n",
    "def predict_target_ratings(train_appids, train_ratings, test_appids):\n",
    "    # If the test_appids list is empty, return an empty list\n",
    "    if len(test_appids) == 0:\n",
    "        return []\n",
    "    # Get dictionary of appids and ratings from train_appids and train_ratings\n",
    "    train_ratings_dict = dict(zip(train_appids, train_ratings))\n",
    "\n",
    "    # For each appid in test_appids, do predict_rating and return a list of tuples of (appid, predicted rating)\n",
    "    predictions = [predict_rating(appid, train_ratings_dict) for appid in test_appids]\n",
    "    return predictions\n",
    "\n",
    "# Create a udf for predict_target_ratings\n",
    "predict_target_ratings_udf = udf(predict_target_ratings, ArrayType(FloatType()))\n",
    "\n",
    "# For each row in the steamid_ratings dataframe, run predict_target_ratings and add the predictions list into a new column\n",
    "steamid_ratings = steamid_ratings.withColumn(\"predictions\", predict_target_ratings_udf(\"train_appids\", \"train_ratings\", \"test_appids\"))\n",
    "\n",
    "# For each column in the predictions column, calculate the rmse\n",
    "def calculate_rmse(predictions, test_ratings):\n",
    "    # If the predictions list is empty, return 0.0\n",
    "    if len(predictions) == 0:\n",
    "        return 0.0\n",
    "    # Calculate the rmse for the predictions list\n",
    "    rmse = 0\n",
    "    prediction_num = 0\n",
    "    for prediction, test_rating in zip(predictions, test_ratings):\n",
    "        # Check if prediction is not 0.0\n",
    "        if prediction != 0.0:\n",
    "            rmse += (prediction - test_rating) ** 2\n",
    "            prediction_num += 1\n",
    "    # Do square root\n",
    "    if prediction_num == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return rmse/prediction_num\n",
    "\n",
    "# Create a udf for calculate_rmse\n",
    "calculate_rmse_udf = udf(calculate_rmse, FloatType())\n",
    "\n",
    "# For each row in the steamid_ratings dataframe, run calculate_rmse and add the rmse into a new column\n",
    "steamid_ratings = steamid_ratings.withColumn(\"rmse\", calculate_rmse_udf(\"predictions\", \"test_ratings\"))\n",
    "\n",
    "# Remove all rows where rmse is 0.0\n",
    "steamid_ratings = steamid_ratings.filter(col(\"rmse\") != 0.0)\n",
    "\n",
    "# Sort the dataframe by rmse in descending order\n",
    "# steamid_ratings = steamid_ratings.sort(col(\"rmse\").desc())\n",
    "\n",
    "# Filter columns where train_appids is less than 3\n",
    "steamid_ratings = steamid_ratings.filter(size(\"train_appids\") > 3)\n",
    "\n",
    "steamid_ratings.show()\n",
    "\n",
    "# Get the rmse for the table\n",
    "rmse = steamid_ratings.select(avg(\"rmse\")).collect()[0][0]\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = math.sqrt(rmse)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Calculate the standard deviation of the ratings\n",
    "ratings_std = test_normalized.select(stddev(\"ratings\")).collect()[0][0]\n",
    "print(f'Standard deviation of ratings: {ratings_std}')\n",
    "\n",
    "# Calculate the percent difference between the rmse and the standard deviation\n",
    "percent_diff = (rmse - ratings_std) / ratings_std * 100\n",
    "print(f'Percent difference: {percent_diff}')\n",
    "\n",
    "# Calculate the standard deviation of rmse\n",
    "rmse_std = steamid_ratings.select(stddev(\"rmse\")).collect()[0][0]\n",
    "print(f'Standard deviation of rmse: {rmse_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+----------------------+----------+\n",
      "|          steamid|        train_appids|       train_ratings|         test_appids|        test_ratings|         predictions|       rmse|global_avg_predictions|  rmse_avg|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+----------------------+----------+\n",
      "|76561197960334000|[70, 80, 220, 240...|[3, 4, 1, 2, 4, 2...|[2200, 8870, 241410]|           [5, 1, 1]|[2.0729234, 2.409...|  4.3366933|  [2.477214, 2.4772...| 3.5762572|\n",
      "|76561197960338000| [240, 380, 400, 10]|        [2, 5, 5, 2]|      [420, 630, 50]|           [5, 3, 4]|[4.2034855, 3.280...| 0.46569654|  [2.477214, 2.4772...| 2.9855435|\n",
      "|76561197960342000|[20, 70, 240, 320...|[2, 2, 2, 2, 2, 4...|        [304930, 30]|              [3, 3]|[3.077104, 2.5227...|0.116869435|  [2.477214, 2.477214]| 0.2733051|\n",
      "|76561197960425000|[30, 70, 400, 440...|[3, 2, 4, 2, 3, 5...|        [630, 11440]|              [4, 5]|[2.0624776, 1.290...|   8.758628|  [2.477214, 2.477214]|  4.341663|\n",
      "|76561197960476000|[50, 220, 240, 34...|[2, 3, 2, 2, 2, 3...|[130, 620, 13250,...|        [2, 1, 5, 5]|[2.682584, 3.7307...|   3.646521|  [2.477214, 2.4772...|  3.784698|\n",
      "|76561197960575000|[240, 500, 550, 5...|     [2, 2, 2, 2, 2]|[440, 219640, 246...|           [2, 5, 2]|     [2.0, 2.0, 2.0]|        3.0|  [2.477214, 2.4772...| 2.2733052|\n",
      "|76561197960816000|[570, 41010, 4268...|[2, 1, 3, 2, 5, 5...|                [10]|                 [2]|          [2.501291]|  0.2512927|            [2.477214]| 0.2277333|\n",
      "|76561197960884000|[240, 440, 8930, ...|[2, 2, 2, 3, 2, 1...|[630, 730, 19030,...|[2, 2, 5, 2, 1, 5...|[1.9354367, 1.419...|  2.7720876|  [2.477214, 2.4772...| 2.2602847|\n",
      "|76561197960944000|[300, 440, 730, 6...|[2, 5, 2, 2, 2, 2...|[240, 49520, 201280]|           [2, 2, 2]|[2.7473874, 3.075...|  0.5714298|  [2.477214, 2.4772...| 0.2277333|\n",
      "|76561197960951000|[220, 240, 400, 5...|[3, 2, 2, 2, 1, 4...|         [96800, 10]|              [2, 2]|[1.3520571, 2.134...| 0.21900842|  [2.477214, 2.477214]| 0.2277333|\n",
      "|76561197961143000|[220, 340, 380, 4...|[3, 2, 2, 2, 2, 2...|[130, 42910, 2118...|     [2, 1, 2, 1, 2]|[2.0, 2.3817005, ...|  1.4645839|  [2.477214, 2.4772...| 1.0095046|\n",
      "|76561197961312000|[70, 220, 400, 44...|[3, 1, 2, 5, 2, 2...|[620, 3590, 12130...|[5, 2, 5, 2, 1, 4...|[2.9786184, 2.266...|  3.1855288|  [2.477214, 2.4772...| 2.7223163|\n",
      "|76561197961354000|[30, 240, 300, 32...|[2, 2, 2, 2, 2, 2...|             [58610]|                 [2]|         [4.1719847]|  4.7175174|            [2.477214]| 0.2277333|\n",
      "|76561197961366000|[60, 70, 211, 240...|[2, 2, 2, 5, 2, 2...|[80, 3590, 42710,...|[2, 2, 3, 1, 2, 2...|[2.5461287, 2.347...| 0.44465694|  [2.477214, 2.4772...|0.51995784|\n",
      "|76561197961433000|[240, 4560, 10180...|[3, 2, 1, 1, 5, 2...|        [730, 32720]|              [2, 1]|[1.6947843, 2.477...|  1.1376591|  [2.477214, 2.477214]| 1.2049474|\n",
      "|76561197961539000|[220, 240, 340, 6...|     [1, 3, 5, 3, 2]|               [130]|                 [2]|          [3.720413]|  2.9598207|            [2.477214]| 0.2277333|\n",
      "|76561197961590000|[240, 440, 730, 7...|[4, 2, 3, 2, 2, 2...|                [80]|                 [2]|         [3.2361054]|  1.5279566|            [2.477214]| 0.2277333|\n",
      "|76561197961685000|[30, 620, 730, 88...|[2, 1, 2, 1, 1, 1...|[570, 44350, 6338...|  [4, 1, 5, 1, 2, 1]|[1.9567645, 1.321...|   2.237144|  [2.477214, 2.4772...| 2.5762572|\n",
      "|76561197961709000|[30, 240, 440, 50...|[2, 2, 3, 3, 2, 3...|            [230410]|                 [2]|         [2.3008916]|0.090535775|            [2.477214]| 0.2277333|\n",
      "|76561197961767000|[30, 70, 240, 300...|[2, 2, 2, 2, 5, 3...|[550, 2200, 8930,...|[3, 2, 3, 2, 2, 4...|[3.3730521, 2.903...|    2.45618|  [2.477214, 2.4772...| 2.0738764|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 359:================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.2615426999557364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the global average of the training dataset\n",
    "global_avg = training.select(avg(\"ratings\")).collect()[0][0]\n",
    "\n",
    "def global_avg_ratings(test_appids):\n",
    "    # If the test_appids list is empty, return an empty list\n",
    "    if len(test_appids) == 0:\n",
    "        return []\n",
    "\n",
    "    # For each appid in test_appids, do predict_rating and return a list of tuples of (appid, predicted rating)\n",
    "    predictions = [global_avg for appid in test_appids]\n",
    "    return predictions\n",
    "\n",
    "# Create a udf for predict_target_ratings\n",
    "global_avg_ratings_udf = udf(global_avg_ratings, ArrayType(FloatType()))\n",
    "\n",
    "# For each row in the steamid_ratings dataframe, run predict_target_ratings and add the predictions list into a new column\n",
    "steamid_ratings = steamid_ratings.withColumn(\"global_avg_predictions\", global_avg_ratings_udf(\"test_appids\"))\n",
    "\n",
    "# For each row in the steamid_ratings dataframe, run calculate_rmse and add the rmse into a new column\n",
    "steamid_ratings = steamid_ratings.withColumn(\"rmse_avg\", calculate_rmse_udf(\"global_avg_predictions\", \"test_ratings\"))\n",
    "\n",
    "# Remove all rows where rmse is 0.0\n",
    "steamid_ratings = steamid_ratings.filter(col(\"rmse_avg\") != 0.0)\n",
    "\n",
    "# Sort the dataframe by rmse in descending order\n",
    "# steamid_ratings = steamid_ratings.sort(col(\"rmse\").desc())\n",
    "\n",
    "steamid_ratings.show()\n",
    "\n",
    "# Get the rmse for the table\n",
    "rmse_avg = steamid_ratings.select(avg(\"rmse_avg\")).collect()[0][0]\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse_avg = math.sqrt(rmse_avg)\n",
    "\n",
    "print(f'RMSE: {rmse_avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for steamid [76561198025744000] and appid [400]: 3.9459605833771616\n"
     ]
    }
   ],
   "source": [
    "# Use predict_rating for appid 400 and user 76561198025744000 and print it nicely\n",
    "print(f'Predicted rating for steamid [{76561198025744000}] and appid [{400}]: {predict_single_rating(76561198025744000, 400, training)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34e120910b4957e5cd755e2a42b93193ccb3c34461c3543db7ce9f360eefaa4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
