{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-item collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, first, mean, row_number, when, udf, stddev_pop, col, lit, collect_list, array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import collect_list, udf, size\n",
    "from pyspark.sql.types import ArrayType, DoubleType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Load dataset\n",
    "2. Split the dataset\n",
    "3. Normalize the dataset playtime into a 1-5 rating scale\n",
    "4. Compute Pearson correlation matrix\n",
    "5. Rating prediction by doing a weighted average of the k most similar items that the user has played before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "The dataset is loaded from MariaDB into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 15:57:37 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.122.1 instead (on interface virbr0)\n",
      "23/04/04 15:57:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 15:57:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('ReadMariaDB') \\\n",
    ".config(\"spark.driver.memory\", \"32g\") \\\n",
    ".config(\"spark.sql.pivotMaxValues\", \"1000000\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "sql = \"select * from 01_sampled_games_2v2 WHERE playtime_forever IS NOT NULL AND playtime_forever > 0\"\n",
    "database = \"steam\"\n",
    "user = \"root\"\n",
    "password = \"example\"\n",
    "server = \"192.168.2.62\"\n",
    "port = 3306\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "# Create a data frame by reading data from Oracle via JDBC\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "df = df.drop(\"playtime_2weeks\", \"dateretrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 88349 rows.\n",
      "+-----------------+-----+----------------+\n",
      "|          steamid|appid|playtime_forever|\n",
      "+-----------------+-----+----------------+\n",
      "|76561197960268000|  300|             109|\n",
      "|76561197960268000| 1300|              94|\n",
      "|76561197960268000| 2100|             110|\n",
      "|76561197960268000| 4000|             152|\n",
      "|76561197960268000| 2600|              59|\n",
      "|76561197960268000| 9000|               2|\n",
      "|76561197960268000| 2300|              40|\n",
      "|76561197960268000| 2200|             210|\n",
      "|76561197960268000| 4500|            1002|\n",
      "|76561197960268000|  400|             380|\n",
      "|76561197960268000|17300|            4312|\n",
      "|76561197960268000|  500|             198|\n",
      "|76561197960268000|22300|             425|\n",
      "|76561197960268000|19900|             111|\n",
      "|76561197960268000|18500|            1160|\n",
      "|76561197960268000|22200|              20|\n",
      "|76561197960268000|35700|             433|\n",
      "|76561197960268000|36000|              27|\n",
      "|76561197960268000|39800|             510|\n",
      "|76561197960268000|32400|              32|\n",
      "+-----------------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"The DataFrame has\", row_count, \"rows.\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into 70% training and 30% test data\n",
    "training, test = df.randomSplit([0.7, 0.3], seed=1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Game 1-5 Rating Normalization\n",
    "For each game, we calculate the mean and standard deviation. We then create buckets for each rating:\n",
    "### Cut points\n",
    "* Cut point 1: mean - std_dev*0.5 if > 0, else 0\n",
    "* Cut point 2: mean\n",
    "* Cut point 3: mean + std_dev*0.5\n",
    "* Cut point 4: mean + std_dev\n",
    "### Ratings\n",
    "* Rating 1: 0 < x < cut point 1\n",
    "* Rating 2: cut point 1 < x < cut point 2\n",
    "* Rating 3: cut point 2 < x < cut point 3\n",
    "* Rating 4: cut point 3 < x < cut point 4\n",
    "* Rating 5: cut point 5 < x < inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+------------------+--------------------+-----------------+-----------+-----------------+------------------+-------+\n",
      "| appid|          steamid|playtime_forever|game_mean_playtime|game_stddev_playtime|      cut_point_1|cut_point_2|      cut_point_3|       cut_point_4|ratings|\n",
      "+------+-----------------+----------------+------------------+--------------------+-----------------+-----------+-----------------+------------------+-------+\n",
      "|263700|76561198071863000|              34|          109.0000|                75.0|             71.5|   109.0000|            146.5|             184.0|      1|\n",
      "|263700|76561197968595000|             184|          109.0000|                75.0|             71.5|   109.0000|            146.5|             184.0|      4|\n",
      "| 49900|76561198062015000|              23|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "| 49900|76561198049923000|              32|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      2|\n",
      "| 49900|76561198045987000|              24|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "| 49900|76561198035231000|               7|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "| 49900|76561198032791000|               8|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "| 49900|76561198024994000|              41|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      2|\n",
      "| 49900|76561198023930000|             128|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      3|\n",
      "| 49900|76561198023872000|             254|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      5|\n",
      "| 49900|76561198020323000|              20|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "| 49900|76561198014889000|               8|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "| 49900|76561198014659000|             132|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      3|\n",
      "| 49900|76561198014641000|             191|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      4|\n",
      "| 49900|76561198014340000|               4|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "| 49900|76561198014116000|             247|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      5|\n",
      "| 49900|76561198012713000|             139|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      3|\n",
      "| 49900|76561198012604000|             374|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      5|\n",
      "| 49900|76561198012100000|             128|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      3|\n",
      "| 49900|76561198011640000|               9|           99.9355|  138.69656727242017|30.58721636378992|    99.9355|169.2837836362101|238.63206727242016|      1|\n",
      "+------+-----------------+----------------+------------------+--------------------+-----------------+-----------+-----------------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the per-game mean and standard deviation of the playtime column\n",
    "game_stats = training.filter(col(\"playtime_forever\") > 0).groupBy(\"appid\").agg(\n",
    "    mean(\"playtime_forever\").alias(\"game_mean_playtime\"),\n",
    "    stddev_pop(\"playtime_forever\").alias(\"game_stddev_playtime\")\n",
    ")\n",
    "training = training.join(game_stats, \"appid\")\n",
    "\n",
    "training = training.withColumn(\"cut_point_1\", when(col(\"game_mean_playtime\") - (col(\"game_stddev_playtime\") * 0.5) > 0, col(\"game_mean_playtime\") - (col(\"game_stddev_playtime\") * 0.5)).otherwise(0))\n",
    "training = training.withColumn(\"cut_point_2\", col(\"game_mean_playtime\"))\n",
    "training = training.withColumn(\"cut_point_3\", col(\"game_mean_playtime\") + (col(\"game_stddev_playtime\") * 0.5))\n",
    "training = training.withColumn(\"cut_point_4\", col(\"game_mean_playtime\") + col(\"game_stddev_playtime\"))\n",
    "\n",
    "training = training.withColumn(\n",
    "    \"ratings\",\n",
    "    when(col(\"playtime_forever\") <= col(\"cut_point_1\"), lit(1))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_1\")) & (col(\"playtime_forever\") <= col(\"cut_point_2\")), lit(2))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_2\")) & (col(\"playtime_forever\") <= col(\"cut_point_3\")), lit(3))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_3\")) & (col(\"playtime_forever\") <= col(\"cut_point_4\")), lit(4))\n",
    "    .otherwise(lit(5))\n",
    ")\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Matrix\n",
    "Since the dataset contains at most ~4500 games, we can expect a 4500^2=81,000,000 sized matrix. Each float entry takes 4 bytes in memory. Therefore, the pearson correlation matrix would take up 324 MB of memory.\n",
    "\n",
    "Since the memory used is relatively small, we will pre-compute the person correlation matrix and store it for use later in the algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we create a list of features for each game (appid) using the playtime_forever of all users who have played the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum length of the lists of ratings values\n",
    "max_len = training.filter(\"ratings IS NOT NULL\") \\\n",
    "    .groupBy('appid').agg(size(collect_list('ratings')).alias('num_playtimes')) \\\n",
    "    .agg({'num_playtimes': 'max'}).collect()[0][0]\n",
    "\n",
    "# Define a UDF to pad lists with zeros\n",
    "pad_zeros = udf(lambda x: x + [0.0]*(max_len-len(x)), ArrayType(DoubleType()))\n",
    "\n",
    "# Create playtime vectors for each game\n",
    "list_to_dense = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "vectors = training.filter(\"ratings IS NOT NULL\")\\\n",
    "    .groupBy('appid').agg(collect_list('ratings'))\\\n",
    "        .withColumn('padded_features', pad_zeros('collect_list(ratings)')) \\\n",
    "        .withColumn('features', list_to_dense('padded_features'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a row number column to the dataframe so that we can match the appid with the row number in the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "|appid|collect_list(ratings)|     padded_features|            features|row_num|\n",
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "|  100| [2, 3, 2, 2, 2, 2...|[null, null, null...|[2.0,3.0,2.0,2.0,...|      1|\n",
      "|  300| [2, 2, 2, 2, 3, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      2|\n",
      "|  400| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      3|\n",
      "|  500| [2, 2, 3, 2, 2, 2...|[null, null, null...|[2.0,2.0,3.0,2.0,...|      4|\n",
      "| 1200| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      5|\n",
      "| 1300| [2, 1, 2, 1, 1, 5...|[null, null, null...|[2.0,1.0,2.0,1.0,...|      6|\n",
      "| 1500| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      7|\n",
      "| 1600| [1, 5, 1, 5, 3, 3...|[null, null, null...|[1.0,5.0,1.0,5.0,...|      8|\n",
      "| 1700| [5, 5, 1, 5, 1, 2...|[null, null, null...|[5.0,5.0,1.0,5.0,...|      9|\n",
      "| 1900| [2, 3, 2, 3, 3, 2...|[null, null, null...|[2.0,3.0,2.0,3.0,...|     10|\n",
      "| 2100| [1, 2, 2, 1, 1, 1...|[null, null, null...|[1.0,2.0,2.0,1.0,...|     11|\n",
      "| 2200| [3, 4, 2, 2, 2, 4...|[null, null, null...|[3.0,4.0,2.0,2.0,...|     12|\n",
      "| 2300| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     13|\n",
      "| 2400| [5, 1, 2, 2, 1, 1...|[null, null, null...|[5.0,1.0,2.0,2.0,...|     14|\n",
      "| 2500| [2, 2, 4, 3, 2, 2...|[null, null, null...|[2.0,2.0,4.0,3.0,...|     15|\n",
      "| 2600| [1, 2, 2, 5, 5, 5...|[null, null, null...|[1.0,2.0,2.0,5.0,...|     16|\n",
      "| 2700| [3, 3, 1, 3, 1, 3...|[null, null, null...|[3.0,3.0,1.0,3.0,...|     17|\n",
      "| 2800| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     18|\n",
      "| 2900|      [5, 2, 1, 2, 2]|[null, null, null...|[5.0,2.0,1.0,2.0,...|     19|\n",
      "| 3000| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     20|\n",
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a row number column to the game matrix\n",
    "windowSpec = Window.orderBy(\"appid\")\n",
    "vectors = vectors.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "vectors.show()\n",
    "\n",
    "# Create a dictionary of appids and row numbers\n",
    "all_row_num = vectors.select(\"appid\", \"row_num\").rdd.collectAsMap()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the correlation method provided by Pyspark, we compute the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.00671087  0.08301802 ... -0.00418148 -0.00418148\n",
      "  -0.00418148]\n",
      " [-0.00671087  1.          0.08396612 ... -0.00362308 -0.00362308\n",
      "  -0.00362308]\n",
      " [ 0.08301802  0.08396612  1.         ...  0.00190252  0.00190252\n",
      "   0.00190252]\n",
      " ...\n",
      " [-0.00418148 -0.00362308  0.00190252 ...  1.          1.\n",
      "   1.        ]\n",
      " [-0.00418148 -0.00362308  0.00190252 ...  1.          1.\n",
      "   1.        ]\n",
      " [-0.00418148 -0.00362308  0.00190252 ...  1.          1.\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "pearson_matrix = Correlation.corr(vectors.orderBy(\"row_num\"), \"features\", \"pearson\")\n",
    "corr_array = pearson_matrix.head()[0].toArray()\n",
    "print(corr_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Prediction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given user (steamid) and game (appid), we can compute the predicted rating of the game (appid) for the user (steamid) using the weighted average of the k most similar games to the game (appid) that the user (steamid) has played.\n",
    "\n",
    "This is the same item-item collaborative filtering formula that was shown in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(appid, user_ratings_dict):\n",
    "\n",
    "    # Get appid row number from the vectors dataframe\n",
    "    appid_row_num = all_row_num[appid]\n",
    "\n",
    "    # Get a list of correlations between the appid and all other games\n",
    "    corr = corr_array[appid_row_num]\n",
    "\n",
    "    # Set the correlation to 0 for appids that the user has not rated\n",
    "    for appid in all_row_num:\n",
    "        if appid not in user_ratings_dict:\n",
    "            corr[all_row_num[appid]] = 0\n",
    "\n",
    "    # Make a list of tuples of (appid, correlation, rating)\n",
    "    corr_list = []\n",
    "    for appid in all_row_num:\n",
    "        if appid in user_ratings_dict:\n",
    "            row_num = all_row_num[appid]\n",
    "            corr_list.append((appid, corr[row_num], user_ratings_dict[appid]))\n",
    "\n",
    "    # Sort the list by correlation\n",
    "    corr_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 most similar appids\n",
    "    top_10 = corr_list[1:11]\n",
    "\n",
    "    # Calculate the weighted average of the top 10 appids\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for appid, corr, rating in top_10:\n",
    "        numerator += corr * rating\n",
    "        denominator += corr\n",
    "    if denominator != 0:\n",
    "        return numerator / denominator\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def predict_single_rating(steamid, appid):\n",
    "    # Get all the user's ratings\n",
    "    user_ratings_dict = training.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "    return predict_rating(appid, user_ratings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for steamid [76561198023872000] and appid [500]: 2.8795349009874287\n"
     ]
    }
   ],
   "source": [
    "# Test the function for a given user and game\n",
    "test_steamid = 76561198023872000\n",
    "test_appid = 500\n",
    "print(f'Predicted rating for steamid [{test_steamid}] and appid [{test_appid}]: {predict_single_rating(test_steamid, test_appid)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n"
     ]
    }
   ],
   "source": [
    "# Get all unique appids\n",
    "all_appids = vectors.select(\"appid\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted top 3 games for steamid [76561198023872000]: [Decimal('45300'), Decimal('47500'), Decimal('43500')]\n"
     ]
    }
   ],
   "source": [
    "def predict_user_ratings(steamid, recommendation_count=3):\n",
    "    # Get all_appids that the user has not rated\n",
    "    user_ratings_dict = training.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "    not_rated = [appid for appid in all_appids if appid not in user_ratings_dict]\n",
    "    not_rated_rdd = spark.sparkContext.parallelize(not_rated)\n",
    "    # Run predict_rating for each appid and output a list of tuples of (appid, predicted rating)\n",
    "    predictions = not_rated_rdd.map(lambda appid: (appid, predict_rating(appid, user_ratings_dict))).collect()\n",
    "    # predictions = [(appid, predict_rating(steamid, appid)) for appid in not_rated]\n",
    "    # Sort the list by predicted rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Return top 3 appids outside of the tuple\n",
    "    return [appid for appid, rating in predictions[:recommendation_count]]\n",
    "\n",
    "print(f'Predicted top 3 games for steamid [{test_steamid}]: {predict_user_ratings(test_steamid)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34e120910b4957e5cd755e2a42b93193ccb3c34461c3543db7ce9f360eefaa4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
