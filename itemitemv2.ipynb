{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-item collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, first, mean, row_number, when, udf, stddev_pop, col, lit, collect_list, array, avg\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import collect_list, udf, size\n",
    "from pyspark.sql.types import ArrayType, DoubleType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Load dataset\n",
    "2. Split the dataset\n",
    "3. Normalize the dataset playtime into a 1-5 rating scale\n",
    "4. Compute Pearson correlation matrix\n",
    "5. Rating prediction by doing a weighted average of the k most similar items that the user has played before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "The dataset is loaded from MariaDB into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ReadMariaDB') \\\n",
    ".config(\"spark.driver.memory\", \"32g\") \\\n",
    ".config(\"spark.sql.pivotMaxValues\", \"1000000\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "sql = \"select * from 01_sampled_games_2 WHERE playtime_forever IS NOT NULL AND playtime_forever > 0\"\n",
    "database = \"steam\"\n",
    "user = \"root\"\n",
    "password = \"example\"\n",
    "server = \"192.168.2.62\"\n",
    "port = 3306\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "# Create a data frame by reading data from Oracle via JDBC\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "df = df.drop(\"playtime_2weeks\", \"dateretrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 408349 rows.\n",
      "+-----------------+-----+----------------+\n",
      "|          steamid|appid|playtime_forever|\n",
      "+-----------------+-----+----------------+\n",
      "|76561197960268000|   10|               8|\n",
      "|76561197960268000|   20|               1|\n",
      "|76561197960268000|   50|            1719|\n",
      "|76561197960268000|   60|               1|\n",
      "|76561197960268000|   70|            1981|\n",
      "|76561197960268000|  130|             175|\n",
      "|76561197960268000|  220|            3873|\n",
      "|76561197960268000|  240|             221|\n",
      "|76561197960268000|  320|               1|\n",
      "|76561197960268000|  280|            1242|\n",
      "|76561197960268000|  300|             109|\n",
      "|76561197960268000|  360|               3|\n",
      "|76561197960268000| 1300|              94|\n",
      "|76561197960268000| 1313|             213|\n",
      "|76561197960268000|  380|             944|\n",
      "|76561197960268000| 2100|             110|\n",
      "|76561197960268000| 4000|             152|\n",
      "|76561197960268000| 3970|             586|\n",
      "|76561197960268000| 2600|              59|\n",
      "|76561197960268000| 6910|            1729|\n",
      "+-----------------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"The DataFrame has\", row_count, \"rows.\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into 70% training and 30% test data\n",
    "training, test = df.randomSplit([0.7, 0.3], seed=1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Game 1-5 Rating Normalization\n",
    "For each game, we calculate the mean and standard deviation. We then create buckets for each rating:\n",
    "### Cut points\n",
    "* Cut point 1: mean - std_dev*0.5 if > 0, else 0\n",
    "* Cut point 2: mean\n",
    "* Cut point 3: mean + std_dev*0.5\n",
    "* Cut point 4: mean + std_dev\n",
    "### Ratings\n",
    "* Rating 1: 0 < x < cut point 1\n",
    "* Rating 2: cut point 1 < x < cut point 2\n",
    "* Rating 3: cut point 2 < x < cut point 3\n",
    "* Rating 4: cut point 3 < x < cut point 4\n",
    "* Rating 5: cut point 5 < x < inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1895:> (0 + 1) / 1][Stage 1896:> (0 + 1) / 1][Stage 1897:> (0 + 1) / 1]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-------+\n",
      "| appid|          steamid|ratings|\n",
      "+------+-----------------+-------+\n",
      "|    10|76561197960389000|      2|\n",
      "|    10|76561197961433000|      1|\n",
      "|  4560|76561197961433000|      1|\n",
      "| 10190|76561197961433000|      1|\n",
      "| 32720|76561197961433000|      1|\n",
      "| 95900|76561197961433000|      5|\n",
      "|    10|76561197961995000|      1|\n",
      "|    10|76561197962565000|      2|\n",
      "|    30|76561197962565000|      2|\n",
      "|    70|76561197962565000|      2|\n",
      "|   440|76561197962565000|      3|\n",
      "|   550|76561197962565000|      2|\n",
      "|   630|76561197962565000|      2|\n",
      "| 42680|76561197962565000|      2|\n",
      "| 42690|76561197962565000|      3|\n",
      "| 42700|76561197962565000|      2|\n",
      "| 42710|76561197962565000|      2|\n",
      "|202990|76561197962565000|      2|\n",
      "|    10|76561197967974000|      1|\n",
      "|   240|76561197970517000|      1|\n",
      "+------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the per-game mean and standard deviation of the playtime column\n",
    "game_stats = training.filter(col(\"playtime_forever\") > 0).groupBy(\"appid\").agg(\n",
    "    mean(\"playtime_forever\").alias(\"game_mean_playtime\"),\n",
    "    stddev_pop(\"playtime_forever\").alias(\"game_stddev_playtime\")\n",
    ")\n",
    "\n",
    "# Calculate the overall playtime average\n",
    "overall_playtime_avg = training.filter(col(\"playtime_forever\") > 0).agg(avg(\"playtime_forever\")).collect()[0][0]\n",
    "\n",
    "# Calculate the per-steamid playtime average\n",
    "user_playtime_avg = training.filter(col(\"playtime_forever\") > 0).groupBy(\"steamid\").agg(avg(\"playtime_forever\")).withColumnRenamed(\"avg(playtime_forever)\", \"user_playtime_avg\")\n",
    "\n",
    "# Join the user_playtime_avg dataframe with the main dataframe\n",
    "training = training.join(user_playtime_avg, \"steamid\")\n",
    "\n",
    "# Join the game_stats dataframe with the main dataframe\n",
    "training = training.join(game_stats, \"appid\")\n",
    "\n",
    "# Calculate the scaling factor based on the ratio of user playtime to overall playtime\n",
    "training = training.withColumn(\"scaling_factor\", col(\"user_playtime_avg\") / overall_playtime_avg)\n",
    "\n",
    "# Calculate the adjusted cut points\n",
    "training = training.withColumn(\"cut_point_1\", when(col(\"game_mean_playtime\") - (col(\"game_stddev_playtime\") * 0.5 * col(\"scaling_factor\")) > 0, col(\"game_mean_playtime\") - (col(\"game_stddev_playtime\") * 0.5 * col(\"scaling_factor\"))).otherwise(0))\n",
    "training = training.withColumn(\"cut_point_2\", col(\"game_mean_playtime\") * col(\"scaling_factor\"))\n",
    "training = training.withColumn(\"cut_point_3\", col(\"game_mean_playtime\") + (col(\"game_stddev_playtime\") * 0.5 * col(\"scaling_factor\")))\n",
    "training = training.withColumn(\"cut_point_4\", col(\"game_mean_playtime\") + col(\"game_stddev_playtime\") * col(\"scaling_factor\"))\n",
    "\n",
    "# Assign ratings based on adjusted cut points\n",
    "training = training.withColumn(\n",
    "    \"ratings\",\n",
    "    when(col(\"playtime_forever\") <= col(\"cut_point_1\"), lit(1))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_1\")) & (col(\"playtime_forever\") <= col(\"cut_point_2\")), lit(2))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_2\")) & (col(\"playtime_forever\") <= col(\"cut_point_3\")), lit(3))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_3\")) & (col(\"playtime_forever\") <= col(\"cut_point_4\")), lit(4))\n",
    "    .otherwise(lit(5))\n",
    ")\n",
    "\n",
    "# Drop the columns that are no longer needed\n",
    "training = training.drop(\"playtime_forever\", \"game_mean_playtime\", \"game_stddev_playtime\", \"user_playtime_avg\", \"scaling_factor\", \"cut_point_1\", \"cut_point_2\", \"cut_point_3\", \"cut_point_4\")\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Matrix\n",
    "Since the dataset contains at most ~4500 games, we can expect a 4500^2=81,000,000 sized matrix. Each float entry takes 4 bytes in memory. Therefore, the pearson correlation matrix would take up 324 MB of memory.\n",
    "\n",
    "Since the memory used is relatively small, we will pre-compute the person correlation matrix and store it for use later in the algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we create a list of features for each game (appid) using the playtime_forever of all users who have played the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the maximum length of the lists of ratings values\n",
    "max_len = training.filter(\"ratings IS NOT NULL\") \\\n",
    "    .groupBy('appid').agg(size(collect_list('ratings')).alias('num_playtimes')) \\\n",
    "    .agg({'num_playtimes': 'max'}).collect()[0][0]\n",
    "\n",
    "# Define a UDF to pad lists with zeros\n",
    "pad_zeros = udf(lambda x: x + [0.0]*(max_len-len(x)), ArrayType(DoubleType()))\n",
    "\n",
    "# Create playtime vectors for each game\n",
    "list_to_dense = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "vectors = training.filter(\"ratings IS NOT NULL\")\\\n",
    "    .groupBy('appid').agg(collect_list('ratings'))\\\n",
    "        .withColumn('padded_features', pad_zeros('collect_list(ratings)')) \\\n",
    "        .withColumn('features', list_to_dense('padded_features'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a row number column to the dataframe so that we can match the appid with the row number in the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "|appid|collect_list(ratings)|     padded_features|            features|row_num|\n",
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "|   10| [1, 1, 3, 2, 2, 1...|[null, null, null...|[1.0,1.0,3.0,2.0,...|      1|\n",
      "|   20| [2, 1, 1, 2, 2, 1...|[null, null, null...|[2.0,1.0,1.0,2.0,...|      2|\n",
      "|   30| [1, 2, 2, 3, 1, 2...|[null, null, null...|[1.0,2.0,2.0,3.0,...|      3|\n",
      "|   40| [2, 2, 2, 2, 2, 1...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      4|\n",
      "|   50| [1, 1, 2, 1, 1, 1...|[null, null, null...|[1.0,1.0,2.0,1.0,...|      5|\n",
      "|   60| [2, 1, 2, 1, 3, 2...|[null, null, null...|[2.0,1.0,2.0,1.0,...|      6|\n",
      "|   70| [2, 1, 2, 1, 3, 2...|[null, null, null...|[2.0,1.0,2.0,1.0,...|      7|\n",
      "|   80| [1, 2, 2, 3, 2, 2...|[null, null, null...|[1.0,2.0,2.0,3.0,...|      8|\n",
      "|   92| [2, 3, 1, 1, 1, 5...|[null, null, null...|[2.0,3.0,1.0,1.0,...|      9|\n",
      "|  100| [5, 2, 2, 2, 1, 1...|[null, null, null...|[5.0,2.0,2.0,2.0,...|     10|\n",
      "|  130| [3, 1, 1, 1, 1, 3...|[null, null, null...|[3.0,1.0,1.0,1.0,...|     11|\n",
      "|  211| [4, 1, 4, 2, 2, 1...|[null, null, null...|[4.0,1.0,4.0,2.0,...|     12|\n",
      "|  220| [3, 2, 5, 1, 5, 2...|[null, null, null...|[3.0,2.0,5.0,1.0,...|     13|\n",
      "|  240| [1, 1, 1, 2, 2, 4...|[null, null, null...|[1.0,1.0,1.0,2.0,...|     14|\n",
      "|  280| [1, 3, 1, 2, 1, 1...|[null, null, null...|[1.0,3.0,1.0,2.0,...|     15|\n",
      "|  300| [2, 2, 2, 1, 2, 1...|[null, null, null...|[2.0,2.0,2.0,1.0,...|     16|\n",
      "|  320| [2, 1, 3, 2, 3, 2...|[null, null, null...|[2.0,1.0,3.0,2.0,...|     17|\n",
      "|  340| [1, 5, 1, 1, 1, 1...|[null, null, null...|[1.0,5.0,1.0,1.0,...|     18|\n",
      "|  360| [1, 3, 1, 2, 5, 1...|[null, null, null...|[1.0,3.0,1.0,2.0,...|     19|\n",
      "|  380| [1, 5, 3, 5, 1, 1...|[null, null, null...|[1.0,5.0,3.0,5.0,...|     20|\n",
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a row number column to the game matrix\n",
    "windowSpec = Window.orderBy(\"appid\")\n",
    "vectors = vectors.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "vectors.show()\n",
    "\n",
    "# Create a dictionary of appids and row numbers\n",
    "all_row_num = vectors.select(\"appid\", \"row_num\").rdd.collectAsMap()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the correlation method provided by Pyspark, we compute the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -3.05231206e-02 -7.62062725e-03 ... -7.57718591e-04\n",
      "  -7.57718591e-04 -7.57718591e-04]\n",
      " [-3.05231206e-02  1.00000000e+00  9.26931449e-02 ... -9.69986298e-03\n",
      "  -9.69986298e-03 -9.69986298e-03]\n",
      " [-7.62062725e-03  9.26931449e-02  1.00000000e+00 ... -7.50437381e-03\n",
      "  -7.50437381e-03 -7.50437381e-03]\n",
      " ...\n",
      " [-7.57718591e-04 -9.69986298e-03 -7.50437381e-03 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [-7.57718591e-04 -9.69986298e-03 -7.50437381e-03 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [-7.57718591e-04 -9.69986298e-03 -7.50437381e-03 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "pearson_matrix = Correlation.corr(vectors.orderBy(\"row_num\"), \"features\", \"pearson\")\n",
    "corr_array = pearson_matrix.head()[0].toArray()\n",
    "print(corr_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Prediction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given user (steamid) and game (appid), we can compute the predicted rating of the game (appid) for the user (steamid) using the weighted average of the k most similar games to the game (appid) that the user (steamid) has played.\n",
    "\n",
    "This is the same item-item collaborative filtering formula that was shown in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(appid, user_ratings_dict, k=10):\n",
    "\n",
    "    # Get appid row number from the vectors dataframe\n",
    "    appid_row_num = all_row_num[appid]\n",
    "\n",
    "    # Get a list of correlations between the appid and all other games\n",
    "    corr = corr_array[appid_row_num]\n",
    "\n",
    "    # Create a dict of appids and correlations\n",
    "    corr_dict = {}\n",
    "    for appid in all_row_num:\n",
    "        if appid in user_ratings_dict:\n",
    "            corr_dict[appid] = corr[all_row_num[appid]]\n",
    "\n",
    "    # Make a list of tuples of (appid, correlation, rating)\n",
    "    corr_list = []\n",
    "    for appid, corr in corr_dict.items():\n",
    "        corr_list.append((appid, corr, user_ratings_dict[appid]))\n",
    "\n",
    "    # Sort the list by correlation\n",
    "    corr_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 most similar appids\n",
    "    top_k = corr_list[1:k+1]\n",
    "\n",
    "    # Calculate the weighted average of the top 10 appids\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for appid, corr, rating in top_k:\n",
    "        numerator += corr * rating\n",
    "        denominator += corr\n",
    "    if denominator != 0:\n",
    "        return numerator / denominator\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def predict_single_rating(steamid, appid):\n",
    "    # Get all the user's ratings\n",
    "    user_ratings_dict = training.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "\n",
    "    # Predict the user's rating for the appid\n",
    "    return predict_rating(appid, user_ratings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1976:> (0 + 1) / 1][Stage 1977:> (0 + 1) / 1][Stage 1978:> (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for steamid [76561198023872000] and appid [500]: 2.4896393222332143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test the function for a given user and game\n",
    "test_steamid = 76561198023872000\n",
    "test_appid = 500\n",
    "print(f'Predicted rating for steamid [{test_steamid}] and appid [{test_appid}]: {predict_single_rating(test_steamid, test_appid)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run in parallel the rating prediction for each game that the user has not played before. We then sort the games by the predicted rating and return the top k games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get all unique appids\n",
    "all_appids = vectors.select(\"appid\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2006:===========================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted top 3 games for steamid [76561198023872000]: [Decimal('229100'), Decimal('224260'), Decimal('233250')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def predict_user_ratings(steamid, recommendation_count=3):\n",
    "    # Get all_appids that the user has not rated\n",
    "    user_ratings_dict = training.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\").rdd.collectAsMap()\n",
    "    not_rated = [appid for appid in all_appids if appid not in user_ratings_dict]\n",
    "    not_rated_rdd = spark.sparkContext.parallelize(not_rated)\n",
    "\n",
    "    # Run predict_rating for each appid and output a list of tuples of (appid, predicted rating)\n",
    "    predictions = not_rated_rdd.map(lambda appid: (appid, predict_rating(appid, user_ratings_dict))).collect()\n",
    "\n",
    "    # Sort the list by predicted rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top 3 appids outside of the tuple\n",
    "    return [appid for appid, rating in predictions[:recommendation_count]]\n",
    "\n",
    "print(f'Predicted top 3 games for steamid [{test_steamid}]: {predict_user_ratings(test_steamid)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34e120910b4957e5cd755e2a42b93193ccb3c34461c3543db7ce9f360eefaa4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
