{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, first, mean, row_number, when, udf, stddev_pop, col, lit, collect_list, array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import collect_list, udf, size\n",
    "from pyspark.sql.types import ArrayType, DoubleType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Load dataset\n",
    "2. Split the dataset\n",
    "3. Normalize the dataset playtime into a 1-5 rating scale\n",
    "4. Compute Pearson correlation matrix\n",
    "5. Rating prediction by doing a weighted average of the k most similar items that the user has played before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "The dataset is loaded from MariaDB into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ReadMariaDB') \\\n",
    ".config(\"spark.driver.memory\", \"32g\") \\\n",
    ".config(\"spark.sql.pivotMaxValues\", \"1000000\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "sql = \"select * from 01_sampled_games_2 WHERE playtime_forever IS NOT NULL AND playtime_forever > 0\"\n",
    "database = \"steam\"\n",
    "user = \"root\"\n",
    "password = \"example\"\n",
    "server = \"192.168.2.62\"\n",
    "port = 3306\n",
    "jdbc_url = f\"jdbc:mysql://{server}:{port}/{database}?permitMysqlScheme\"\n",
    "jdbc_driver = \"org.mariadb.jdbc.Driver\"\n",
    "\n",
    "# Create a data frame by reading data from Oracle via JDBC\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"query\", sql) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", jdbc_driver) \\\n",
    "    .load()\n",
    "\n",
    "df = df.drop(\"playtime_2weeks\", \"dateretrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 408349 rows.\n",
      "+-----------------+-----+----------------+\n",
      "|          steamid|appid|playtime_forever|\n",
      "+-----------------+-----+----------------+\n",
      "|76561197960268000|   10|               8|\n",
      "|76561197960268000|   20|               1|\n",
      "|76561197960268000|   50|            1719|\n",
      "|76561197960268000|   60|               1|\n",
      "|76561197960268000|   70|            1981|\n",
      "|76561197960268000|  130|             175|\n",
      "|76561197960268000|  220|            3873|\n",
      "|76561197960268000|  240|             221|\n",
      "|76561197960268000|  320|               1|\n",
      "|76561197960268000|  280|            1242|\n",
      "|76561197960268000|  300|             109|\n",
      "|76561197960268000|  360|               3|\n",
      "|76561197960268000| 1300|              94|\n",
      "|76561197960268000| 1313|             213|\n",
      "|76561197960268000|  380|             944|\n",
      "|76561197960268000| 2100|             110|\n",
      "|76561197960268000| 4000|             152|\n",
      "|76561197960268000| 3970|             586|\n",
      "|76561197960268000| 2600|              59|\n",
      "|76561197960268000| 6910|            1729|\n",
      "+-----------------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"The DataFrame has\", row_count, \"rows.\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the data into 70% training and 30% test data\n",
    "training, test = df.randomSplit([0.7, 0.3], seed=1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Game 1-5 Rating Normalization\n",
    "For each game, we calculate the mean and standard deviation. We then create buckets for each rating:\n",
    "### Cut points\n",
    "* Cut point 1: mean - std_dev*0.5 if > 0, else 0\n",
    "* Cut point 2: mean\n",
    "* Cut point 3: mean + std_dev*0.5\n",
    "* Cut point 4: mean + std_dev\n",
    "### Ratings\n",
    "* Rating 1: 0 < x < cut point 1\n",
    "* Rating 2: cut point 1 < x < cut point 2\n",
    "* Rating 3: cut point 2 < x < cut point 3\n",
    "* Rating 4: cut point 3 < x < cut point 4\n",
    "* Rating 5: cut point 5 < x < inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1043:>               (0 + 1) / 1][Stage 1044:>               (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+------------------+--------------------+-----------------+-----------+------------------+------------------+-------+\n",
      "| appid|          steamid|playtime_forever|game_mean_playtime|game_stddev_playtime|      cut_point_1|cut_point_2|       cut_point_3|       cut_point_4|ratings|\n",
      "+------+-----------------+----------------+------------------+--------------------+-----------------+-----------+------------------+------------------+-------+\n",
      "|206480|76561197960268000|             247|          905.1463|  1534.3106112371472|137.9909943814264|   905.1463|1672.3016056185736|2439.4569112371473|      2|\n",
      "|218230|76561197960293000|            2491|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      3|\n",
      "|221640|76561197960306000|              10|          151.9692|   296.4365657519208|3.750917124039603|   151.9692| 300.1874828759604| 448.4057657519208|      2|\n",
      "|218230|76561197960342000|              93|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      2|\n",
      "|221640|76561197960417000|              20|          151.9692|   296.4365657519208|3.750917124039603|   151.9692| 300.1874828759604| 448.4057657519208|      2|\n",
      "|206480|76561197960481000|            2243|          905.1463|  1534.3106112371472|137.9909943814264|   905.1463|1672.3016056185736|2439.4569112371473|      4|\n",
      "|218230|76561197960564000|             849|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      2|\n",
      "|221640|76561197960795000|              14|          151.9692|   296.4365657519208|3.750917124039603|   151.9692| 300.1874828759604| 448.4057657519208|      2|\n",
      "|206480|76561197961017000|              75|          905.1463|  1534.3106112371472|137.9909943814264|   905.1463|1672.3016056185736|2439.4569112371473|      1|\n",
      "|221640|76561197961141000|               1|          151.9692|   296.4365657519208|3.750917124039603|   151.9692| 300.1874828759604| 448.4057657519208|      1|\n",
      "|218230|76561197961258000|             437|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      2|\n",
      "|221640|76561197961312000|               8|          151.9692|   296.4365657519208|3.750917124039603|   151.9692| 300.1874828759604| 448.4057657519208|      2|\n",
      "|221640|76561197961347000|               1|          151.9692|   296.4365657519208|3.750917124039603|   151.9692| 300.1874828759604| 448.4057657519208|      1|\n",
      "|218230|76561197961530000|             663|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      2|\n",
      "|218230|76561197961767000|            3536|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      3|\n",
      "|221640|76561197961822000|              21|          151.9692|   296.4365657519208|3.750917124039603|   151.9692| 300.1874828759604| 448.4057657519208|      2|\n",
      "|206480|76561197962197000|              18|          905.1463|  1534.3106112371472|137.9909943814264|   905.1463|1672.3016056185736|2439.4569112371473|      1|\n",
      "|218230|76561197962551000|              93|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      2|\n",
      "|218230|76561197962599000|            3089|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      3|\n",
      "|218230|76561197962907000|             890|         1967.5565|   4198.422921177205|              0.0|  1967.5565|4066.7679605886024| 6165.979421177205|      2|\n",
      "+------+-----------------+----------------+------------------+--------------------+-----------------+-----------+------------------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the per-game mean and standard deviation of the playtime column\n",
    "game_stats = training.filter(col(\"playtime_forever\") > 0).groupBy(\"appid\").agg(\n",
    "    mean(\"playtime_forever\").alias(\"game_mean_playtime\"),\n",
    "    stddev_pop(\"playtime_forever\").alias(\"game_stddev_playtime\")\n",
    ")\n",
    "training = training.join(game_stats, \"appid\")\n",
    "\n",
    "training = training.withColumn(\"cut_point_1\", when(col(\"game_mean_playtime\") - (col(\"game_stddev_playtime\") * 0.5) > 0, col(\"game_mean_playtime\") - (col(\"game_stddev_playtime\") * 0.5)).otherwise(0))\n",
    "training = training.withColumn(\"cut_point_2\", col(\"game_mean_playtime\"))\n",
    "training = training.withColumn(\"cut_point_3\", col(\"game_mean_playtime\") + (col(\"game_stddev_playtime\") * 0.5))\n",
    "training = training.withColumn(\"cut_point_4\", col(\"game_mean_playtime\") + col(\"game_stddev_playtime\"))\n",
    "\n",
    "training = training.withColumn(\n",
    "    \"ratings\",\n",
    "    when(col(\"playtime_forever\") <= col(\"cut_point_1\"), lit(1))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_1\")) & (col(\"playtime_forever\") <= col(\"cut_point_2\")), lit(2))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_2\")) & (col(\"playtime_forever\") <= col(\"cut_point_3\")), lit(3))\n",
    "    .when((col(\"playtime_forever\") > col(\"cut_point_3\")) & (col(\"playtime_forever\") <= col(\"cut_point_4\")), lit(4))\n",
    "    .otherwise(lit(5))\n",
    ")\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Matrix\n",
    "Since the dataset contains at most ~4500 games, we can expect a 4500^2=81,000,000 sized matrix. Each float entry takes 4 bytes in memory. Therefore, the pearson correlation matrix would take up 324 MB of memory.\n",
    "\n",
    "Since the memory used is relatively small, we will pre-compute the person correlation matrix and store it for use later in the algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we create a list of features for each game (appid) using the playtime_forever of all users who have played the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the maximum length of the lists of ratings values\n",
    "max_len = training.filter(\"ratings IS NOT NULL\") \\\n",
    "    .groupBy('appid').agg(size(collect_list('ratings')).alias('num_playtimes')) \\\n",
    "    .agg({'num_playtimes': 'max'}).collect()[0][0]\n",
    "\n",
    "# Define a UDF to pad lists with zeros\n",
    "pad_zeros = udf(lambda x: x + [0.0]*(max_len-len(x)), ArrayType(DoubleType()))\n",
    "\n",
    "# Create playtime vectors for each game\n",
    "list_to_dense = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "vectors = training.filter(\"ratings IS NOT NULL\")\\\n",
    "    .groupBy('appid').agg(collect_list('ratings'))\\\n",
    "        .withColumn('padded_features', pad_zeros('collect_list(ratings)')) \\\n",
    "        .withColumn('features', list_to_dense('padded_features'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a row number column to the dataframe so that we can match the appid with the row number in the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "|appid|collect_list(ratings)|     padded_features|            features|row_num|\n",
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "|   10| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      1|\n",
      "|   20| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      2|\n",
      "|   30| [2, 2, 2, 2, 2, 3...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      3|\n",
      "|   40| [2, 2, 2, 3, 2, 2...|[null, null, null...|[2.0,2.0,2.0,3.0,...|      4|\n",
      "|   50| [5, 2, 3, 3, 2, 4...|[null, null, null...|[5.0,2.0,3.0,3.0,...|      5|\n",
      "|   60| [2, 3, 2, 2, 3, 2...|[null, null, null...|[2.0,3.0,2.0,2.0,...|      6|\n",
      "|   70| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      7|\n",
      "|   80| [2, 2, 2, 2, 5, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|      8|\n",
      "|   92| [5, 3, 3, 2, 3, 1...|[null, null, null...|[5.0,3.0,3.0,2.0,...|      9|\n",
      "|  100| [2, 2, 2, 2, 2, 5...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     10|\n",
      "|  130| [2, 2, 4, 2, 2, 2...|[null, null, null...|[2.0,2.0,4.0,2.0,...|     11|\n",
      "|  211| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     12|\n",
      "|  220| [5, 2, 3, 3, 1, 1...|[null, null, null...|[5.0,2.0,3.0,3.0,...|     13|\n",
      "|  240| [2, 2, 2, 2, 3, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     14|\n",
      "|  280| [5, 2, 1, 3, 2, 2...|[null, null, null...|[5.0,2.0,1.0,3.0,...|     15|\n",
      "|  300| [2, 2, 2, 2, 2, 3...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     16|\n",
      "|  320| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     17|\n",
      "|  340| [2, 2, 2, 2, 2, 2...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     18|\n",
      "|  360| [2, 2, 2, 2, 2, 3...|[null, null, null...|[2.0,2.0,2.0,2.0,...|     19|\n",
      "|  380| [4, 2, 2, 2, 2, 2...|[null, null, null...|[4.0,2.0,2.0,2.0,...|     20|\n",
      "+-----+---------------------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a row number column to the game matrix\n",
    "windowSpec = Window.orderBy(\"appid\")\n",
    "vectors = vectors.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "vectors.show()\n",
    "\n",
    "# Create a dictionary of appids and row numbers\n",
    "all_row_num = vectors.select(\"appid\", \"row_num\").rdd.collectAsMap()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the correlation method provided by Pyspark, we compute the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -7.49560139e-03  3.70704555e-02 ... -1.63025619e-03\n",
      "  -1.63025619e-03 -1.63025619e-03]\n",
      " [-7.49560139e-03  1.00000000e+00  1.38129132e-01 ...  1.57316375e-04\n",
      "   1.57316375e-04  1.57316375e-04]\n",
      " [ 3.70704555e-02  1.38129132e-01  1.00000000e+00 ...  1.99965389e-03\n",
      "   1.99965389e-03  1.99965389e-03]\n",
      " ...\n",
      " [-1.63025619e-03  1.57316375e-04  1.99965389e-03 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [-1.63025619e-03  1.57316375e-04  1.99965389e-03 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [-1.63025619e-03  1.57316375e-04  1.99965389e-03 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "pearson_matrix = Correlation.corr(vectors.orderBy(\"row_num\"), \"features\", \"pearson\")\n",
    "corr_array = pearson_matrix.head()[0].toArray()\n",
    "print(corr_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Prediction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given user (steamid) and game (appid), we can compute the predicted rating of the game (appid) for the user (steamid) using the weighted average of the k most similar games to the game (appid) that the user (steamid) has played.\n",
    "\n",
    "This is the same item-item collaborative filtering formula that was shown in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(steamid, appid):\n",
    "    # Get all the user's ratings\n",
    "    user_ratings = training.filter(col(\"steamid\") == steamid).select(\"appid\", \"ratings\")\n",
    "\n",
    "    # Get appid row number from the vectors dataframe\n",
    "    appid_row_num = vectors.filter(col(\"appid\") == appid).select(\"row_num\").collect()[0][0]\n",
    "\n",
    "    # Get a list of correlations between the appid and all other games\n",
    "    corr = corr_array[appid_row_num]\n",
    "\n",
    "    # Create a dictionary of appids and ratings for the user\n",
    "    user_ratings_dict = user_ratings.rdd.collectAsMap()\n",
    "\n",
    "    # Set the correlation to 0 for appids that the user has not rated\n",
    "    for appid in all_row_num:\n",
    "        if appid not in user_ratings_dict:\n",
    "            corr[all_row_num[appid]] = 0\n",
    "\n",
    "    # Make a list of tuples of (appid, correlation, rating)\n",
    "    corr_list = []\n",
    "    for appid in all_row_num:\n",
    "        if appid in user_ratings_dict:\n",
    "            row_num = all_row_num[appid]\n",
    "            corr_list.append((appid, corr[row_num], user_ratings_dict[appid]))\n",
    "\n",
    "    # Sort the list by correlation\n",
    "    corr_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 most similar appids\n",
    "    top_10 = corr_list[1:11]\n",
    "\n",
    "    # Calculate the weighted average of the top 10 appids\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for appid, corr, rating in top_10:\n",
    "        numerator += corr * rating\n",
    "        denominator += corr\n",
    "    if denominator != 0:\n",
    "        return numerator / denominator\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1101:>               (0 + 1) / 1][Stage 1102:>               (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for steamid [76561198023872000] and appid [500]: 2.4888994312569337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test the function for a given user and game\n",
    "test_steamid = 76561198023872000\n",
    "test_appid = 500\n",
    "print(f'Predicted rating for steamid [{test_steamid}] and appid [{test_appid}]: {predict_rating(test_steamid, test_appid)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34e120910b4957e5cd755e2a42b93193ccb3c34461c3543db7ce9f360eefaa4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
